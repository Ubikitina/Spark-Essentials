{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "01fdb178-6a8f-4553-a96e-136e0744f2fd",
     "showTitle": false,
     "title": ""
    },
    "id": "8Z0h3dF9Vg4X"
   },
   "source": [
    "# Download Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a2be725e-f792-4224-9a10-152bcde68fa2",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KBDin-0sXgyI",
    "outputId": "681ac0b0-ac2c-4b10-edba-bec4cf21660c"
   },
   "outputs": [],
   "source": [
    "%sh \n",
    "curl -O 'https://raw.githubusercontent.com/masfworld/datahack_docker/master/zeppelin/data/el_quijote.txt'\n",
    "curl -O 'https://raw.githubusercontent.com/masfworld/datahack_docker/master/zeppelin/data/frankenstein.txt'\n",
    "curl -O 'https://raw.githubusercontent.com/masfworld/datahack_docker/master/zeppelin/data/characters.csv'\n",
    "curl -O 'https://raw.githubusercontent.com/masfworld/datahack_docker/master/zeppelin/data/species.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e843b7b0-932c-43a5-8dd2-290066fbc555",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.rm(\"/dataset/streaming\", True)\n",
    "dbutils.fs.mkdirs(\"/dataset/streaming\")\n",
    "dbutils.fs.cp('file:/databricks/driver/el_quijote.txt','dbfs:/dataset/streaming/el_quijote.txt')\n",
    "dbutils.fs.cp('file:/databricks/driver/frankenstein.txt','dbfs:/dataset/frankenstein.txt')\n",
    "dbutils.fs.cp('file:/databricks/driver/characters.csv','dbfs:/dataset/characters.csv')\n",
    "dbutils.fs.cp('file:/databricks/driver/species.csv','dbfs:/dataset/species.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "30c82f68-1e2a-470c-a3d4-d3749b5f7171",
     "showTitle": false,
     "title": ""
    },
    "id": "02Zwm3NRXS_I"
   },
   "source": [
    "# Structured Streaming\n",
    "Structured Streaming is a high-level API for stream processing that became production-ready in Spark 2.2. It is scalable and fault-tolerant, and it is built on the Spark SQL engine. Structured Streaming allows you to take the same operations that you perform in batch mode using Sparkâ€™s structured APIs, and run them in a streaming fashion. This can reduce latency and allow for incremental processing. For more information, visit [Databricks - Structured Streaming](https://www.databricks.com/glossary/what-is-structured-streaming).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3c8056f5-8e52-4033-9076-42ebbc3b56c3",
     "showTitle": false,
     "title": ""
    },
    "id": "h1o6f6QOjTcZ"
   },
   "source": [
    "## Example 1 - Read a streaming folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1c4cc931-d240-4f02-927f-3847aea33fa8",
     "showTitle": false,
     "title": ""
    },
    "id": "yjPpxRyJYA1h"
   },
   "source": [
    "Read a streaming folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cbbde9c8-3c73-4621-93eb-70c0a041c2e5",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HnbafeFCVk8d",
    "outputId": "7c3b356f-c746-4021-e4c5-8ff656ec2809"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Read streaming data from the specified directory\n",
    "lines = spark \\\n",
    "  .readStream \\\n",
    "  .format(\"text\") \\\n",
    "  .load(\"/dataset/streaming/\")\n",
    "\n",
    "# Split the lines into words and create a DataFrame with a column named \"word\"\n",
    "words = lines.select(\n",
    "    explode(split(col(\"value\"), \" \")).alias(\"word\"),    \n",
    ")\n",
    "\n",
    "# Group the words and count their occurrences, then sort by count in descending order\n",
    "groupedWords = words \\\n",
    "  .groupBy(\"word\") \\\n",
    "  .count() \\\n",
    "  .sort(col(\"count\").desc())\n",
    "\n",
    "# Define a streaming query that writes the complete output to an in-memory table\n",
    "query = groupedWords \\\n",
    "  .writeStream \\\n",
    "  .outputMode(\"complete\") \\\n",
    "  .format(\"memory\") \\\n",
    "  .queryName(\"testquijote\")\n",
    "\n",
    "# Start the first streaming query\n",
    "query.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "263af737-f5a6-4817-97bf-28a833471928",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Group the words and count their occurrences\n",
    "# No sorting is applied here because sorting is not supported on streaming DataFrames/Datasets unless it is on an aggregated DataFrame/Dataset in Complete output mode\n",
    "groupedWords2 = words \\\n",
    "  .groupBy(\"word\") \\\n",
    "  .count()\n",
    "\n",
    "# Define another streaming query that writes only updated output to an in-memory table\n",
    "query2 = groupedWords2 \\\n",
    "  .writeStream \\\n",
    "  .outputMode(\"update\") \\\n",
    "  .format(\"memory\") \\\n",
    "  .queryName(\"testquijote2\")\n",
    "\n",
    "# Start the second streaming query\n",
    "query2.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40499f2f-ebf8-443b-8259-ded286beddaa",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HTGfY-5LskwQ",
    "outputId": "1b1d5b9a-98b8-40c7-a1de-c581ba6cf5c6"
   },
   "outputs": [],
   "source": [
    "# Execute an SQL query on the in-memory table \"testquijote\" and show the first 10 results\n",
    "spark.sql(\"select * from testquijote limit 10\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7e7f6ee7-9b0e-4234-a34e-52e3dd74274c",
     "showTitle": false,
     "title": ""
    },
    "id": "ZpcAQtSUtteu"
   },
   "source": [
    "**Now let's copy a new file into the streaming directory to see that the in-memory table captures the changes.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70304641-cd10-425c-88c2-71eaa6cf72fd",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-y7xXjmbs44o",
    "outputId": "5858546f-0eb7-45ae-f6e8-70cea63a14b3"
   },
   "outputs": [],
   "source": [
    "# Copy the file \"frankenstein.txt\" from \"/dataset/\" to the \"/dataset/streaming/\" directory\n",
    "dbutils.fs.cp('/dataset/frankenstein.txt','/dataset/streaming/')\n",
    "\n",
    "# List the files in the \"/dataset/streaming/\" directory to verify the copy\n",
    "dbutils.fs.ls('/dataset/streaming')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0806622a-e261-43fc-823e-81ad1273cb35",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3qpygEOHtHwh",
    "outputId": "61650a2d-226e-4c90-b9cc-6025c9be8218"
   },
   "outputs": [],
   "source": [
    "# Execute an SQL query on the in-memory table \"testquijote\" and show the first 10 results\n",
    "spark.sql(\"select * from testquijote limit 10\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "117fd5d2-2ed8-4978-94f2-bce6e7572835",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "For example, we see that the word \"the\" appears, which did not appear before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "53031db7-c970-489d-aa2b-5b333fd203a6",
     "showTitle": false,
     "title": ""
    },
    "id": "yzU_4EjAjZgh"
   },
   "source": [
    "## Exercise 1 - Filter words with less than 4 characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "647a7405-9578-4785-8b1e-6f8ef0d745e6",
     "showTitle": false,
     "title": ""
    },
    "id": "sV-NchsMsHvF"
   },
   "source": [
    "Using the example 1 code, filter out all words with less than 4 characters\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b1cb859-e745-4917-99ac-75754364d4aa",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SgireGq6YWEj",
    "outputId": "2fce361b-8994-48a8-f039-e82a2517143b"
   },
   "outputs": [],
   "source": [
    "# Execute an SQL query on the in-memory table \"testquijote\" to select words with length greater than 4 and show the first 10 results\n",
    "spark.sql(\"select * from testquijote where length(word) > 4 limit 10\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc1d3bb6-3eb2-44da-8ab5-43486dd1435f",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "prPdHV2ehnWI",
    "outputId": "e6243e75-c7b7-4600-8eed-985f96c41edd"
   },
   "outputs": [],
   "source": [
    "# Filter words with length greater than 4, group them, count their occurrences, and sort by count in descending order\n",
    "groupedWords_2 = words \\\n",
    "  .filter(length(\"word\") > 4) \\\n",
    "  .groupBy(\"word\") \\\n",
    "  .count() \\\n",
    "  .sort(col(\"count\").desc())\n",
    "\n",
    "# Define a streaming query that writes the filtered and grouped words to an in-memory table\n",
    "query_2 = groupedWords_2 \\\n",
    "  .writeStream \\\n",
    "  .outputMode(\"complete\") \\\n",
    "  .format(\"memory\") \\\n",
    "  .queryName(\"testquijote_2\")\n",
    "\n",
    "# Start the streaming query\n",
    "query_2.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "234b92c2-4cd2-4b29-afdd-d67c31bc6c5b",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8_BLn2cSiA85",
    "outputId": "36353e66-c27d-46b8-ec77-77d5b7a1d307"
   },
   "outputs": [],
   "source": [
    "# Execute an SQL query on the in-memory table \"testquijote_2\" and show the first 10 results\n",
    "spark.sql(\"select * from testquijote_2 limit 10\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "467a9d0d-0b47-4281-8f48-fc220b280128",
     "showTitle": false,
     "title": ""
    },
    "id": "7j_LLcGeo2-j"
   },
   "source": [
    "## Example 2 - Read a CSV file and apply a schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "398f5fca-6d52-4385-9c15-cf118fd3a3b2",
     "showTitle": false,
     "title": ""
    },
    "id": "UgDfrBiqpAq_"
   },
   "source": [
    "Reading a CSV file, applying a schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa133222-485c-41a6-be1d-f1c4361fb2c7",
     "showTitle": false,
     "title": ""
    },
    "id": "qDBtt-33pH2B"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "# Define the schema for the CSV files\n",
    "schema = StructType([\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"height\", StringType(), True),\n",
    "    StructField(\"hair_color\", StringType(), True),\n",
    "    StructField(\"skin_color\", StringType(), True),\n",
    "    StructField(\"eye_color\", StringType(), True),\n",
    "    StructField(\"birth_year\", StringType(), True),\n",
    "    StructField(\"gender\", StringType(), True),\n",
    "    StructField(\"homeworld\", StringType(), True),\n",
    "    StructField(\"species\", StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "93bdbed1-b792-4fe8-a0ad-71c6d1297bfc",
     "showTitle": false,
     "title": ""
    },
    "id": "HG1ObxVnpSD1"
   },
   "outputs": [],
   "source": [
    "# Read streaming data from the specified directory with the given schema\n",
    "lines = spark.readStream \\\n",
    "  .format(\"csv\") \\\n",
    "  .schema(schema) \\\n",
    "  .load(\"/dataset/charac*.csv\") \\\n",
    "  .withColumn(\"current_timestamp\", current_timestamp())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7258ba36-1c8c-49db-a0ad-14e28918acb2",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AoRAS3YWpV6m",
    "outputId": "80262564-e192-434d-89ba-baa8622df171"
   },
   "outputs": [],
   "source": [
    "# Define a streaming query that writes the data to an in-memory table\n",
    "query = lines.writeStream \\\n",
    "  .outputMode(\"update\") \\\n",
    "  .format(\"memory\") \\\n",
    "  .queryName(\"charac\") \\\n",
    "\n",
    "# Start running the query that prints the running counts to the console\n",
    "query.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea452cf5-a2f2-4235-93e1-e32c7eef6d18",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AqDjqB0lpYrY",
    "outputId": "29f50c1a-637b-437b-82fc-1bda24028124"
   },
   "outputs": [],
   "source": [
    "# Execute an SQL query on the in-memory table \"charac\" and show the first 10 results\n",
    "spark.sql(\"select * from charac limit 10\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2ab6458b-2dbf-400f-bc34-634d7d277926",
     "showTitle": false,
     "title": ""
    },
    "id": "rPVTLyFgrCJZ"
   },
   "source": [
    "# Windowing\n",
    "**Windowing** is a powerful feature in Apache Spark that allows for set-based computations (such as aggregations) or other operations over subsets of events within a specified time period. This is particularly useful for processing time-series data, streaming data, and real-time analytics, where operations need to be performed over a continuous stream of data points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "de364b88-c2da-428e-a6f5-26ee67a82a5f",
     "showTitle": false,
     "title": ""
    },
    "id": "42PI1onm9kIh"
   },
   "source": [
    "## Example 3 - 5 seconds fix window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6430db2c-6c77-40f8-9b7c-7891e0747ec7",
     "showTitle": false,
     "title": ""
    },
    "id": "nbf8iJP0AlkN"
   },
   "source": [
    "Read in streaming files `el_quijote.txt` and `frankestein.txt`. Applying a 5 seconds fix window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "254529f7-5e8f-44f7-b174-1a2044d4b9fa",
     "showTitle": false,
     "title": ""
    },
    "id": "PUIqs27uAvM9"
   },
   "outputs": [],
   "source": [
    "# Remove the \"/dataset/books\" directory and all its contents if it exists\n",
    "dbutils.fs.rm('/dataset/books', True)\n",
    "\n",
    "# Create a new directory at \"/dataset/books\"\n",
    "dbutils.fs.mkdirs(\"/dataset/books\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce92bfae-57f6-41a5-a2a0-b5a25300c0af",
     "showTitle": false,
     "title": ""
    },
    "id": "9579ZquMBCm9"
   },
   "outputs": [],
   "source": [
    "# Copy the file \"el_quijote.txt\" from \"/dataset/streaming/\" to \"/dataset/books/\"\n",
    "dbutils.fs.cp('/dataset/streaming/el_quijote.txt', '/dataset/books/')\n",
    "\n",
    "# List the files in the \"/dataset/books/\" directory to verify the copy\n",
    "dbutils.fs.ls('/dataset/books/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0419ffa-f3cf-40fa-b4b7-7e99498be558",
     "showTitle": false,
     "title": ""
    },
    "id": "pKPZ4kc2BUdA"
   },
   "outputs": [],
   "source": [
    "# Read streaming data from the \"/dataset/books/\" directory\n",
    "words = spark \\\n",
    "  .readStream \\\n",
    "  .format(\"text\") \\\n",
    "  .load(\"/dataset/books/\") \\\n",
    "  .select(explode(split(col(\"value\"), \" \")).alias(\"word\"), col(\"current_timestamp\")) \\\n",
    "  .withColumn(\"current_timestamp\", current_timestamp()) # Add a column with the current timestamp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e7741cd4-b5a7-4c91-a6f1-52d10d4c1018",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "`explode(split(col(\"value\"), \" \")).alias(\"word\")`: Split lines into words and add a \"word\" column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce654916-5728-4b45-afd4-11faf78dc0ce",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZYpiFeIbBgRW",
    "outputId": "52f246f7-7180-4b46-f0b0-c63e9558e456"
   },
   "outputs": [],
   "source": [
    "# Group the words by a 5-second window and count their occurrences, then sort by count in descending order\n",
    "windowedCounts = words \\\n",
    "  .groupBy(\n",
    "      window(col(\"current_timestamp\"), \"5 seconds\"),\n",
    "      col(\"word\")\n",
    "  ) \\\n",
    "  .count() \\\n",
    "  .sort(col(\"count\").desc())\n",
    "\n",
    "# Define a streaming query that writes the windowed counts to an in-memory table\n",
    "query = windowedCounts \\\n",
    "  .writeStream \\\n",
    "  .outputMode(\"complete\") \\\n",
    "  .format(\"memory\") \\\n",
    "  .queryName(\"test_windowing_books\")\n",
    "\n",
    "# Start the streaming query\n",
    "query.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a22ae9f4-1030-4468-859f-13e24bd92718",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_Av3IdgbBs0C",
    "outputId": "033b3c23-c1a3-4acb-ecfc-43c085dadb36"
   },
   "outputs": [],
   "source": [
    "# Execute an SQL query on the in-memory table \"test_windowing_books\" and show the first 10 results, displaying full content\n",
    "spark.sql(\"select * from test_windowing_books limit 10\").show(20, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bda30368-1d25-49ef-bdef-09e4185b8b43",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Tj-Wkc67Dyni",
    "outputId": "37f68704-e9c2-4d12-ee5c-d24beed6f071"
   },
   "outputs": [],
   "source": [
    "# Execute an SQL query to group the results by window and count the number of occurrences in each window, displaying full content\n",
    "spark.sql(\"select window, count(*) from test_windowing_books group by window\").show(20, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fce91979-0c86-4e41-865d-b6f4a838dacf",
     "showTitle": false,
     "title": ""
    },
    "id": "QwLEx9SdB0jd"
   },
   "outputs": [],
   "source": [
    "# Copy the file \"el_quijote.txt\" from \"/dataset/streaming/\" to \"/dataset/books/\" again\n",
    "dbutils.fs.cp('/dataset/streaming/el_quijote.txt', '/dataset/books/el_quijote2.txt')\n",
    "\n",
    "# List the files in the \"/dataset/books/\" directory to verify the copy\n",
    "dbutils.fs.ls('/dataset/books/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64bdd546-774c-47d7-919a-66914a91a58f",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PcI1-3-aEFyk",
    "outputId": "b293151c-33d4-4b55-f915-15558a043990"
   },
   "outputs": [],
   "source": [
    "# Execute an SQL query to group the results by window and count the number of occurrences in each window again, displaying full content\n",
    "spark.sql(\"select window, count(*) from test_windowing_books group by window\").show(20, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f18a5321-c9fb-4f89-ae0b-2dbb487200af",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "We can see that after copying a new file to the path, we now have two windows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "edd84dfe-f803-4124-b48f-f596a7030d4b",
     "showTitle": false,
     "title": ""
    },
    "id": "jkR3V9kYE6IF"
   },
   "source": [
    "## Exercise 2 - 2 seconds fix window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "466820d2-7e63-4857-9d9f-5c256a86b130",
     "showTitle": false,
     "title": ""
    },
    "id": "JmTOOQQUE8KV"
   },
   "source": [
    "Get the number of different species classifications from `species.csv`\n",
    "- Split the file in multiple parts, create a new folder, inserting each part in the folder one by one\n",
    "- Group the result in 2 seconds fix window\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eab3de4e-a6fb-4344-bcb1-15a9450ae328",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QnHdFUvEESoK",
    "outputId": "353d9dd6-0d12-40d3-d5d4-042bfa35ff8b"
   },
   "outputs": [],
   "source": [
    "# Remove the \"/dataset/species_splitted\" directory and all its contents if it exists\n",
    "dbutils.fs.rm('/dataset/species_splitted', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "922b266d-2b8c-46bc-a121-3edf7a87d8d0",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Dg1lH6HwFhc5",
    "outputId": "9ca24767-ff8d-4485-ccfe-b1513aa5418f"
   },
   "outputs": [],
   "source": [
    "# Read the CSV file \"/dataset/species.csv\" with inferred schema and header\n",
    "data_species = spark \\\n",
    "  .read \\\n",
    "  .load(\"/dataset/species.csv\", format=\"csv\", inferSchema=\"true\", header=\"true\")\n",
    "\n",
    "# Repartition the DataFrame into 3 partitions and write it back as CSV files to \"/dataset/species_splitted\"\n",
    "data_species \\\n",
    "  .repartition(3) \\\n",
    "  .write.csv(\"/dataset/species_splitted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0736d00e-42e4-47f9-b612-8aa99ff57aa7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# List the files in the \"/dataset/species_splitted\" directory to verify the operation\n",
    "dbutils.fs.ls('/dataset/species_splitted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b14c87c-47f9-47c9-9fc4-acb18c574796",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Remove the \"/dataset/species\" directory and all its contents if it exists\n",
    "dbutils.fs.rm('/dataset/species', True)\n",
    "# Create a new directory at \"/dataset/species\"\n",
    "dbutils.fs.mkdirs('/dataset/species')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "266bf724-a9e0-459d-b03d-ce8d0c8bb68e",
     "showTitle": false,
     "title": ""
    },
    "id": "q7c06TuRFnXs"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "# Define the schema for the species data\n",
    "schema_species = StructType([\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"classification\", StringType(), True),\n",
    "    StructField(\"designation\", StringType(), True),\n",
    "    StructField(\"skin_colors\", StringType(), True),\n",
    "    StructField(\"hair_colors\", StringType(), True),\n",
    "    StructField(\"eye_colors\", StringType(), True),\n",
    "    StructField(\"average_lifespan\", StringType(), True),\n",
    "    StructField(\"language\", StringType(), True),\n",
    "    StructField(\"homeworld\", StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c7e1ad28-fac6-44dd-bdb9-1e89a0b661f9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Enable schema inference for streaming queries\n",
    "spark.sql(\"set spark.sql.streaming.schemaInference=true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49be1a3e-f8d8-4ea5-a5eb-554129eb3e02",
     "showTitle": false,
     "title": ""
    },
    "id": "rNmDCBhOGLN7"
   },
   "outputs": [],
   "source": [
    "# Read streaming data from the \"/dataset/species/\" directory with the given schema\n",
    "rows = spark.readStream \\\n",
    "  .format(\"csv\") \\\n",
    "  .schema(schema_species) \\\n",
    "  .load(\"/dataset/species/\") \\\n",
    "  .withColumn(\"current_timestamp\", current_timestamp())\n",
    "\n",
    "# Group the rows by a 2-second window and classification, then count the occurrences\n",
    "windowedCounts_species = rows.groupBy(\n",
    "      window(col(\"current_timestamp\"), \"2 seconds\"),\n",
    "      col(\"classification\")\n",
    "  ).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bba345b7-675e-497b-b803-baf1d07930ca",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b863sgbXG69X",
    "outputId": "5e8010d2-5938-43b3-aabb-5c6209c70ae8"
   },
   "outputs": [],
   "source": [
    "# Define a streaming query that writes the windowed counts to an in-memory table\n",
    "query_species = windowedCounts_species.writeStream \\\n",
    "  .outputMode(\"update\") \\\n",
    "  .format(\"memory\") \\\n",
    "  .queryName(\"windowing_species\") \n",
    "\n",
    "# Start the streaming query\n",
    "query_species.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41730bd4-8a73-4773-9ebe-a6f3171739df",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8HVPR6HQIAvd",
    "outputId": "661c5f69-7110-4d46-b023-935120758c86"
   },
   "outputs": [],
   "source": [
    "# Execute an SQL query on the in-memory table \"windowing_species\" and show the first 30 results, displaying full content\n",
    "spark.sql(\"select * from windowing_species\").show(30, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71623488-2766-4f4f-8073-a3a380fe71c4",
     "showTitle": false,
     "title": ""
    },
    "id": "60gqAX9SH6C9"
   },
   "outputs": [],
   "source": [
    "# Copy a specific partition file from \"/dataset/species_splitted/\" to \"/dataset/species/\"\n",
    "dbutils.fs.cp('/dataset/species_splitted/part-00002-tid-3182637813745946561-4c082602-0398-426b-98b6-8bd6a6250458-3205-1-c000.csv', '/dataset/species/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aac33e92-2f25-44ec-8024-208556a37114",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UdEVq3qPHy_b",
    "outputId": "aa2cc860-5b6f-4167-8c01-3560beec5b10"
   },
   "outputs": [],
   "source": [
    "# Execute an SQL query on the in-memory table \"windowing_species\" and show the first 30 results, displaying full content\n",
    "spark.sql(\"select * from windowing_species\").show(30, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f868d45b-bf3b-4e53-8f0d-17e88ab7914e",
     "showTitle": false,
     "title": ""
    },
    "id": "ix3uggQzIQkR"
   },
   "outputs": [],
   "source": [
    "# Copy additional partition files from \"/dataset/species_splitted/\" to \"/dataset/species/\"\n",
    "dbutils.fs.cp('/dataset/species_splitted/part-00000-tid-3182637813745946561-4c082602-0398-426b-98b6-8bd6a6250458-3203-1-c000.csv', '/dataset/species/')\n",
    "dbutils.fs.cp('/dataset/species_splitted/part-00001-tid-3182637813745946561-4c082602-0398-426b-98b6-8bd6a6250458-3204-1-c000.csv', '/dataset/species/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "acc0183a-d4ba-4a30-84ea-f1ca62be76af",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s2lm2X0-IXVU",
    "outputId": "da1a25ae-7bb7-44fb-cd3e-29b3a915364f"
   },
   "outputs": [],
   "source": [
    "# Execute an SQL query on the in-memory table \"windowing_species\" and show the first 30 results, displaying full content\n",
    "spark.sql(\"select * from windowing_species\").show(30, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fe769950-5de0-41cf-a387-33bd52e9f3a9",
     "showTitle": false,
     "title": ""
    },
    "id": "8i1s62r-hDDL"
   },
   "source": [
    "# Stream - Stream Join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a88f03cc-4f50-47e6-a40c-57513a1c07a5",
     "showTitle": false,
     "title": ""
    },
    "id": "ETU4lxgBhHqE"
   },
   "source": [
    "## Exercise 3 - Inner Join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "19493c72-a2b8-47c9-9555-b1faf6fb8acc",
     "showTitle": false,
     "title": ""
    },
    "id": "V6vE8gzAh7Bz"
   },
   "source": [
    "Update the following code to build an inner join between `df_left` and `df_right` Dataframes.\n",
    "- Set the output mode to `append`\n",
    "- Set 2 hours watermark for both streams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f3895100-21c8-43c0-9acb-0c1d026ec0ce",
     "showTitle": false,
     "title": ""
    },
    "id": "wbUjICTnIeOl"
   },
   "outputs": [],
   "source": [
    "# Create a streaming DataFrame generating data at a rate of 3 rows per second\n",
    "df_left = (\n",
    "    spark\n",
    "    .readStream\n",
    "    .format(\"rate\")\n",
    "    .option(\"rowsPerSecond\", 3)\n",
    "    .load()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5362e93c-3a45-4fac-941f-1bfb1be99301",
     "showTitle": false,
     "title": ""
    },
    "id": "mWS1VnWkiSET"
   },
   "outputs": [],
   "source": [
    "# Create another streaming DataFrame generating data at a rate of 3 rows per second\n",
    "df_right = (\n",
    "    spark\n",
    "    .readStream\n",
    "    .format(\"rate\")\n",
    "    .option(\"rowsPerSecond\", 3)\n",
    "    .load()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab82cd9d-e5c9-4c16-b167-bd46eaa3f666",
     "showTitle": false,
     "title": ""
    },
    "id": "YHAohKgwiVID"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Modify the df_left DataFrame by adding random columns \"left_key\" and \"left_value\" with values between 1 and 10\n",
    "# Also, add a watermark to the \"timestamp\" column with a 2-hour delay\n",
    "df_left_modified = (\n",
    "    df_left\n",
    "    .withColumn(\"left_key\", ceil(rand() * 10))\n",
    "    .withColumn(\"left_value\", ceil(rand() * 10))\n",
    "    .withWatermark(\"timestamp\", \"2 hours\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e7f23a3a-a5cd-4e1b-97d4-4028a9fd32cf",
     "showTitle": false,
     "title": ""
    },
    "id": "kjZRhXoUizIT"
   },
   "outputs": [],
   "source": [
    "# Modify the df_right DataFrame by adding random columns \"right_key\" and \"right_value\" with values between 1 and 10\n",
    "df_right_modified = (\n",
    "    df_right\n",
    "    .withColumn(\"right_key\", ceil(rand() * 10))\n",
    "    .withColumn(\"right_value\", ceil(rand() * 10))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "551bcaec-71ca-4b53-9faf-4015a3b969fb",
     "showTitle": false,
     "title": ""
    },
    "id": "0gJ3NZ-Oi7wc"
   },
   "outputs": [],
   "source": [
    "# Join the modified DataFrames on the condition where \"left_key\" equals \"right_key\"\n",
    "df_joined = (\n",
    "    df_left_modified\n",
    "    .join(df_right_modified, df_left_modified.left_key == df_right_modified.right_key)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "707aac4e-3002-46c0-8bbd-efa3165e715e",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SVO2yg4EjcCg",
    "outputId": "bee3f371-931a-4542-85d1-dc10d7baea6b"
   },
   "outputs": [],
   "source": [
    "# Display the joined DataFrame in a streaming query\n",
    "(\n",
    "df_joined\n",
    " .display()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c0dc9e9d-a115-4f71-8310-67da8f002d67",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "We execute this SQL query to see the joined values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c9ca8051-1a65-427e-8b48-72f8b7509157",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create the Temporary View\n",
    "df_joined.createOrReplaceTempView(\"test_joined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b50476f0-719a-4895-9cbf-88249fc75d18",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7wRZE3Ryj8ms",
    "outputId": "4d3959fc-6bce-42c7-b4fc-93e0bd279d72"
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * \n",
    "FROM test_joined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce49c56b-e805-4cab-b23b-e42ef5cb3dc5",
     "showTitle": false,
     "title": ""
    },
    "id": "xPoFXsEUkC2W"
   },
   "outputs": [],
   "source": [
    "# Add watermarking to both DataFrames on the \"timestamp\" column with a 2-hour delay\n",
    "df_left_watermarked = df_left_modified.withWatermark(\"timestamp\", \"2 hours\")\n",
    "df_right_watermarked = df_right_modified.withWatermark(\"timestamp\", \"2 hours\")\n",
    "\n",
    "# Rename the \"timestamp\" column in df_left_modified to \"timestamp_left\"\n",
    "# Perform a left join with df_right_modified on the condition where \"left_key\" equals \"right_key\"\n",
    "# and the \"timestamp\" in df_right_modified is within 1 hour of \"timestamp_left\"\n",
    "df_left_watermarked.withColumnRenamed(\"timestamp\",\"timestamp_left\")\\\n",
    "    .join(\n",
    "        df_right_watermarked, expr(\"\"\"\n",
    "            left_key = right_key AND\n",
    "            timestamp <= timestamp_left + interval 1 hour\n",
    "        \"\"\"), \n",
    "        how='left')\\\n",
    "    .display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "64b10247-22a2-421a-aa72-9317ad3dde13",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "I added watermarking to both dataframes `df_left_watermarked` and `df_right_watermarked` because otherwise I got an error in the join:\n",
    "\n",
    "`AnalysisException: Stream-stream LeftOuter join between two streaming DataFrame/Datasets is not supported without a watermark in the join keys, or a watermark on the nullable side and an appropriate range condition;`"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 2309605403314200,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "3 Spark Streaming",
   "widgets": {}
  },
  "colab": {
   "collapsed_sections": [
    "42PI1onm9kIh",
    "jkR3V9kYE6IF"
   ],
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
