{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b6ef4018-4842-40f0-860c-74a3cbdc3ddb",
     "showTitle": false,
     "title": ""
    },
    "id": "rPVTLyFgrCJZ"
   },
   "source": [
    "# Structured Streaming with Apache Kafka"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "943ee4d5-0464-43e8-ad4f-2ed555521614",
     "showTitle": false,
     "title": ""
    },
    "id": "42PI1onm9kIh"
   },
   "source": [
    "## Example 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7aab45e2-350f-46cc-9544-a381ba62914c",
     "showTitle": false,
     "title": ""
    },
    "id": "nbf8iJP0AlkN"
   },
   "source": [
    "Reading a Kafka topic in AWS.\n",
    "Before executing this code, replace `kafka:9094` by the right bootstrap server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "397f8f56-69a7-4283-82b2-aff575691525",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PUIqs27uAvM9",
    "outputId": "eb110574-cde1-4d33-f953-cbeb3c24e5dc"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import from_json, col\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "# Read streaming data from Kafka\n",
    "df = spark \\\n",
    "  .readStream \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", \"34.139.223.68:9094\") \\\n",
    "  .option(\"subscribe\", \"toots\") \\\n",
    "  .load()\n",
    "  \n",
    "# Define the schema for parsing JSON data\n",
    "schema = StructType(\n",
    "    [\n",
    "        StructField('id', StringType(), True),\n",
    "        StructField('content', StringType(), True),\n",
    "        StructField('created_at', StringType(), True),\n",
    "        StructField('account', StringType(), True)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Print the schema of the DataFrame\n",
    "df.printSchema()\n",
    "\n",
    "# Parse the value column from Kafka as JSON and expand it into separate columns\n",
    "dataset = df.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\", \"timestamp\") \\\n",
    "    .withColumn(\"value\", from_json(\"value\", schema)) \\\n",
    "    .select(col('key'), col(\"timestamp\"), col('value.*'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "42f33783-f54a-45dc-a87d-ca5d73652e0f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Last parse: the entire operation converts the raw Kafka streaming data (`df`) into a more structured format (`dataset`).\n",
    "1. **Initial `selectExpr` Transformation**:\n",
    "   - `df.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\", \"timestamp\")`: This selects and casts columns from the DataFrame `df`.\n",
    "     - `\"CAST(key AS STRING)\"`: Casts the column `key` to a string type. This is typically done to ensure compatibility or to handle data in a specific format.\n",
    "     - `\"CAST(value AS STRING)\"`: Casts the column `value` to a string type. In many cases, Kafka streams data as byte arrays, so casting it to a string is necessary for further processing.\n",
    "     - `\"timestamp\"`: Presumably, this refers to the existing `timestamp` column in `df`, which is included as-is.\n",
    "\n",
    "2. **Parsing JSON with `from_json`**:\n",
    "   - `.withColumn(\"value\", from_json(\"value\", schema))`: This adds or replaces the `value` column by parsing its content as JSON using the specified `schema`.\n",
    "     - `\"value\"`: Refers to the column name `value` which contains JSON data.\n",
    "     - `from_json(\"value\", schema)`: Uses the `from_json` function to parse JSON data from the `value` column according to the provided `schema`.\n",
    "     - `schema`: Represents the predefined structure (`StructType`) that defines the expected fields (`StructField`) and their types in the JSON data.\n",
    "\n",
    "3. **Final `select` Transformation**:\n",
    "   - `.select(col('key'), col(\"timestamp\"), col('value.*'))`: Selects columns from the DataFrame `df` after parsing JSON.\n",
    "     - `col('key')`: Selects the `key` column as-is.\n",
    "     - `col(\"timestamp\")`: Selects the `timestamp` column as-is.\n",
    "     - `col('value.*')`: Selects all columns (`*`) from the `value` struct column that was parsed from JSON in the previous step. This expands the struct column into individual columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9bd57d8f-594e-4274-8ab4-04ecef66e5f4",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FFE9-1mVb0oL",
    "outputId": "289e8c2a-40c5-43c9-974e-878ef6fd9a4a"
   },
   "outputs": [],
   "source": [
    "# Specify the output mode as 'append' (only new rows added to the result table)\n",
    "# Define the output sink format as 'memory' (store the result in-memory table)\n",
    "# Option to truncate long strings in the output table (set to 'false' to display full content)\n",
    "# Assign a query name for the streaming query (to be referenced in Spark SQL)\n",
    "# Start the streaming query\n",
    "dataset.writeStream \\\n",
    " .outputMode(\"append\") \\\n",
    " .format(\"memory\") \\\n",
    " .option(\"truncate\", \"false\") \\\n",
    " .queryName(\"toots_topic\") \\\n",
    " .start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4c9c2722-ea9d-44ca-91dc-6e35a6086d68",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "The above block of code sets up a streaming pipeline in Spark Structured Streaming that reads data (`dataset`), processes it in append mode, stores the results in-memory, ensures full string content is displayed, assigns a query name, and starts the streaming execution. `writeStream` is a method used to define how the streaming data should be written to an external sink in a continuous and streaming manner. \n",
    "\n",
    "It enables real-time data processing and analysis, making the results immediately available for querying using Spark SQL or other downstream applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4307908c-d816-40ce-a4af-ae8a9a6d6060",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FeYb_B40b4Yz",
    "outputId": "6d0a9dfe-2150-48dc-c859-fe8c0d10dbd7"
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT\n",
    "  *\n",
    "FROM\n",
    "  toots_topic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e5e566c2-0d7f-4240-80d3-dfe7a2479735",
     "showTitle": false,
     "title": ""
    },
    "id": "jkR3V9kYE6IF"
   },
   "source": [
    "## Exercise 1 - Sliding window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "42bca667-4e8e-4ba3-ac27-a05e270a1861",
     "showTitle": false,
     "title": ""
    },
    "id": "JmTOOQQUE8KV"
   },
   "source": [
    "Apply a sliding window each minute, 5 minutes of duration, grouping by `server`. A server in Mastodon is the domain in account column\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "211de108-0686-41d1-b2d5-d32a7232e58b",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QnHdFUvEESoK",
    "outputId": "52ec66fd-b954-40fc-ec8e-9128631209b9"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, window, split, when\n",
    "\n",
    "# Displaying grouped and counted data\n",
    "#    Extracts the server part from 'account' column if it contains '@'\n",
    "#    Groups data by a 5-minute sliding window and 'server'\n",
    "#    Counts occurrences within each window and server group\n",
    "#    Displays the result\n",
    "(\n",
    "dataset\n",
    "  .withColumn(\"server\", when(col(\"account\").contains(\"@\"), split(col(\"account\"), \"@\").getItem(1))\n",
    "                   .otherwise(None))\n",
    "  .groupBy(window(col(\"timestamp\"), \"5 minutes\", \"1 minutes\"), col(\"server\"))\n",
    "  .count()\n",
    "  .display()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4fbca664-e483-4d14-81f0-2c89e8d38299",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, window, split, when\n",
    "\n",
    "# Writing streaming data with update mode\n",
    "#    Extracts the server part from 'account' column if it contains '@'\n",
    "#    Groups data by a 5-minute sliding window and 'server'\n",
    "#    Counts occurrences within each window and server group\n",
    "#    Specifies update mode for streaming write\n",
    "#    Sets the output sink format to in-memory table\n",
    "#    Ensures full content display in the in-memory table\n",
    "#    Assigns a name to the streaming query\n",
    "#    Starts the streaming query\n",
    "(\n",
    "dataset\n",
    "  .withColumn(\"server\", when(col(\"account\").contains(\"@\"), split(col(\"account\"), \"@\").getItem(1))\n",
    "                   .otherwise(None))\n",
    "  .groupBy(window(col(\"timestamp\"), \"5 minutes\", \"1 minutes\"), col(\"server\"))\n",
    "  .count()\n",
    "  .writeStream \\\n",
    "  .outputMode(\"update\") \\\n",
    "  .format(\"memory\") \\\n",
    "  .option(\"truncate\", \"false\") \\\n",
    "  .queryName(\"toots_update_topic\") \\\n",
    "  .start()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2109698b-beaa-4037-befb-479faed9d80b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from toots_update_topic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "83f10342-c437-40e2-9e95-638bb40fd9dd",
     "showTitle": false,
     "title": ""
    },
    "id": "2W5pcAqFdk0h"
   },
   "source": [
    "## Exercise 2 - Get the last 5 min number of toots each minute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "880a447c-b4ff-44b2-84a7-5b0a49aff56c",
     "showTitle": false,
     "title": ""
    },
    "id": "hDW6jxpsdoGI"
   },
   "source": [
    "Each minute, get the number of toots received in last 5 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "207d1de2-70d0-4b93-8f91-1f2d96a9f8c6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Writing windowed data with update mode\n",
    "#    Groups data by a 5-minute sliding window\n",
    "#    ounts occurrences within each window\n",
    "#    Specifies update mode for streaming write\n",
    "#    Sets the output sink format to in-memory table\n",
    "#    Assigns a name to the streaming query\n",
    "#    Starts the streaming query\n",
    "dataset.groupBy(window(col(\"timestamp\"), \"5 minutes\", \"1 minutes\")) \\\n",
    "    .count() \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"update\") \\\n",
    "    .format(\"memory\") \\\n",
    "    .queryName(\"toots_windowed_2\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4adeb964-7c68-4cef-bf41-c0c0a714065a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT\n",
    "  *\n",
    "FROM\n",
    "  toots_windowed_2\n",
    "ORDER BY\n",
    "  count DESC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fa94a729-6891-485a-8ecf-56d2a2b12247",
     "showTitle": false,
     "title": ""
    },
    "id": "pxJyQip9exxX"
   },
   "source": [
    "## Exercise 3 - Get top words in 1 min slots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4b5d3bb8-a955-4f24-bc2d-384e4923be96",
     "showTitle": false,
     "title": ""
    },
    "id": "5wbL-LCuezVu"
   },
   "source": [
    "Get top words with more than 3 letters in 1 minute slots\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed29cef7-184c-4bfe-bb8c-7c758eae7c5b",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VQbOueaAeDX0",
    "outputId": "cf9b4a2a-2873-4ff9-fcd9-a68be926747b"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lower, explode, length\n",
    "\n",
    "# Displaying word counts within 1-minute windows\n",
    "#    Selects columns key, value, and timestamp, casting key and value as strings\n",
    "#    Parses the value column from JSON format using the specified schema\n",
    "#    Selects key, timestamp, and all columns from the parsed JSON value\n",
    "#    Splits the content column by spaces, then explodes it into rows of words, preserving the timestamp\n",
    "#    Filters out words with a length less than or equal to 3\n",
    "dataset = df.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\", \"timestamp\") \\\n",
    "    .withColumn(\"value\", from_json(\"value\", schema)) \\\n",
    "    .select(col('key'), col(\"timestamp\"), col('value.*')) \\\n",
    "    .select(explode(split(col(\"content\"), \" \")).alias(\"word\"), \"timestamp\") \\\n",
    "    .filter(length(col(\"word\")) > 3) # Filters out words with length less than or equal to 3\n",
    "\n",
    "# Groups data by a 1-minute sliding window and 'word'\n",
    "# Counts occurrences of each word within each window and displays the result\n",
    "dataset.groupBy(window(col(\"timestamp\"), \"1 minutes\"), col(\"word\")) \\\n",
    "    .count() \\\n",
    "    .display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "079ea0e1-410c-4aab-8ac6-75784186f51a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Clean up DBFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec352ab9-9a46-4649-b81c-6ffad69908c5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%scala\n",
    "// Clean up\n",
    "val PATH = \"dbfs:/tmp/\" // Define the base path for cleanup\n",
    "\n",
    "// List all files and directories under the specified path\n",
    "// Extract only the names of the files and directories\n",
    "// Iterate through each file and delete it recursively\n",
    "dbutils.fs.ls(PATH)\n",
    "            .map(_.name)\n",
    "            .foreach((file: String) => dbutils.fs.rm(PATH + file, true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "15e39dad-93c8-436e-99fe-995c22c2f9b9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 3838321522709465,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "4 Spark Streaming - Kafka",
   "widgets": {}
  },
  "colab": {
   "collapsed_sections": [],
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
