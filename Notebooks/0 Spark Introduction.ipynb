{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4d98bb0e-3a74-4a00-9c98-e79e76c0c21d",
     "showTitle": false,
     "title": ""
    },
    "id": "8Z0h3dF9Vg4X"
   },
   "source": [
    "# Download Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9af166e6-3548-4448-8f6b-f480600446af",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Use the curl command to download files from specified URLs and save them in the current directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "621a3631-07bb-4ab2-9751-4e26ea3ae04d",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KBDin-0sXgyI",
    "outputId": "775fe7fd-05f3-42e2-f533-657265d94bb9"
   },
   "outputs": [],
   "source": [
    "%sh \n",
    "curl -O 'https://raw.githubusercontent.com/masfworld/datahack_docker/master/zeppelin/data/frankenstein.txt'\n",
    "curl -O 'https://raw.githubusercontent.com/masfworld/datahack_docker/master/zeppelin/data/el_quijote.txt'\n",
    "curl -O 'https://raw.githubusercontent.com/masfworld/datahack_docker/master/zeppelin/data/characters.csv'\n",
    "curl -O 'https://raw.githubusercontent.com/masfworld/datahack_docker/master/zeppelin/data/planets.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "18dd61f3-34e9-4a9f-bce0-8b0cb06c5335",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "This code lists the contents of the directory `/databricks/driver/` in a Databricks environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "83369930-8275-4908-8361-322fcc2d3893",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sh\n",
    "ls /databricks/driver/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "868765d7-8519-4ec8-b3d2-75a23b9271af",
     "showTitle": false,
     "title": ""
    },
    "id": "02Zwm3NRXS_I"
   },
   "source": [
    "# RDD\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "99ee8402-97f5-4601-9418-7a33f025079b",
     "showTitle": false,
     "title": ""
    },
    "id": "h1o6f6QOjTcZ"
   },
   "source": [
    "## Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d8d6dcf3-0492-498b-b4ec-41a5061cfcba",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Example 1 - Create a RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fe53fefd-059b-4051-8e94-4ee10993f0b6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Reads the contents of frankenstein.txt into an RDD and displays the first line of the file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ddebe479-9469-4fec-8d02-d2c8171ca574",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "HnbafeFCVk8d",
    "outputId": "ad6de7c8-a32f-496f-a4a7-11c7944d5b79"
   },
   "outputs": [],
   "source": [
    "textFile = spark.sparkContext.textFile('file:/databricks/driver/frankenstein.txt')\n",
    "display(textFile.first())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1c153a94-caf2-4b28-b35f-0dd30c2d5229",
     "showTitle": false,
     "title": ""
    },
    "id": "9a00GmwOZmM2"
   },
   "source": [
    "\n",
    "### Creation of paralelized collection\n",
    "This is a fast way to create a RDD:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "866f2d35-264e-4b0e-85ef-c8b5d36270a9",
     "showTitle": false,
     "title": ""
    },
    "id": "yzU_4EjAjZgh"
   },
   "source": [
    "### Example 2 - Parallelize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b96c0561-0a81-4c7d-822d-4b067fc16fab",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "1. **Creates an RDD**: It initializes an RDD named `distData` with the list of integers `[25, 20, 15, 10, 5]` using the `parallelize` method of Spark's `SparkContext`.\n",
    "\n",
    "2. **Reduces the RDD**: It applies the `reduce` method to sum all the elements in the RDD. The lambda function `lambda x, y: x + y` specifies that the reduction operation should be summing the elements.\n",
    "\n",
    "3. **Displays the result**: It uses the `display` function to show the result of the reduction, which is the sum of the elements in the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "269b892b-661a-4183-b029-aac02f027a5b",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SgireGq6YWEj",
    "outputId": "663050af-0a22-4ddf-8047-49bf9fcd552a"
   },
   "outputs": [],
   "source": [
    "distData = spark.sparkContext.parallelize([25, 20, 15, 10, 5])\n",
    "display(distData.reduce(lambda x ,y: x + y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8561ba27-4763-4516-a25a-85d9c5204291",
     "showTitle": false,
     "title": ""
    },
    "id": "bX0FXU7JawRm"
   },
   "source": [
    "### Exercise 1 - Count the number of lines\n",
    "Count the number of lines for `el_quijote.txt` file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d2fdaa8-4dc4-490b-a3cb-9e087ae8f61b",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mCDo_-PiaEVl",
    "outputId": "7b4c1fd0-524b-4158-a184-71fd10df1049"
   },
   "outputs": [],
   "source": [
    "# Load the text file 'el_quijote.txt' into an RDD named 'textfile_quijote'\n",
    "textfile_quijote = spark.sparkContext.textFile(\"file:/databricks/driver/el_quijote.txt\")\n",
    "\n",
    "# Count the number of lines in the RDD and print the result\n",
    "print(\"Number of lines: \" + str(textfile_quijote.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7c11ff6a-d8d8-44f5-8bc6-82e220a60129",
     "showTitle": false,
     "title": ""
    },
    "id": "prvVhMD4a5o7"
   },
   "source": [
    "### Exercise 2 - Print the first line\n",
    "Print the first line of the file `el_quijote.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea658870-ab08-484b-812f-086b5ed7efb4",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "1vgL2Upsa-Qg",
    "outputId": "cf77fc55-5cc8-4883-c3c3-00846c918389"
   },
   "outputs": [],
   "source": [
    "display(textfile_quijote.first())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "237513b1-ff1e-49f5-b6e6-1e1f96a15dac",
     "showTitle": false,
     "title": ""
    },
    "id": "wAxxBSrBb92y"
   },
   "source": [
    "## Transformations and Actions in RDDs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d2a838bb-b9db-42d0-b51b-2aad461d144f",
     "showTitle": false,
     "title": ""
    },
    "id": "D-jBGb_acVuZ"
   },
   "source": [
    "### Actions\n",
    "Actions trigger the execution of transformations to produce a result. They perform computations and send the results back to the driver program or save them to an external storage system. Examples include:\n",
    "  - `count()`, which returns the number of elements in the RDD.\n",
    "  - `collect()`, which returns all the elements of the RDD to the driver.\n",
    "  - `saveAsTextFile()`, which writes the data to a text file.\n",
    "  - `reduce()`\n",
    "\n",
    "**Usage**: Actions are used to either save a result to some location or display it.\n",
    "> Be very cautious with actions; we should avoid `collect()` in our production applications as it can lead to an out-of-memory exception."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aa45b677-e9ae-4112-bbb4-a6a151c51559",
     "showTitle": false,
     "title": ""
    },
    "id": "4fc-rQBNjnNi"
   },
   "source": [
    "#### Example 3 - Count and First"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62a053de-ab28-4a15-8885-9c1f2974ee55",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qvxep4yubxtC",
    "outputId": "b1ac7e5b-03b0-43b0-d8d0-99997f50fb1a"
   },
   "outputs": [],
   "source": [
    "print(textFile.count()) # Number of elements in the RDD\n",
    "print(textFile.first()) # First element of the RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0d3492a3-2c30-401f-9187-8863715579a0",
     "showTitle": false,
     "title": ""
    },
    "id": "FYhM504ycl9K"
   },
   "source": [
    "### Transformations\n",
    "- **Operations over RDDs that return a new RDD**: Transformations are operations that create a new RDD from an existing one. \n",
    "- **Lazy Evaluation**: They are lazily evaluated, meaning they only define a new RDD without immediately computing it. \n",
    "  - Only computed when an action requires a result to be returned to the driver program.\n",
    "  - Note: Some transformations like `sortByKey` are not lazy.\n",
    "- Examples include:\n",
    "  - `map()`, which applies a function to each element in the RDD.\n",
    "  - `filter()`, which returns a new RDD containing only the elements that satisfy a given condition.\n",
    "\n",
    "> Note: Consider that SparkSQL transformations are other different kind of transformations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "781e8375-4b0e-4d50-bd3c-ac4018589bd0",
     "showTitle": false,
     "title": ""
    },
    "id": "irxzzmfwjyYi"
   },
   "source": [
    "#### Example 4 - ReduceByKey and SortByKey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "827affde-8a0d-410a-9eaa-beae34df18e7",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MpkUk7t9cfoL",
    "outputId": "47818e05-5383-457b-bef8-5f8ce91f026f"
   },
   "outputs": [],
   "source": [
    "# ReduceByKey\n",
    "\n",
    "# Load the text file 'frankenstein.txt' into an RDD named 'lines'\n",
    "lines = spark.sparkContext.textFile(\"file:/databricks/driver/frankenstein.txt\")\n",
    "\n",
    "# Map each line in the RDD to a pair (line, 1), creating an RDD of pairs\n",
    "pairs = lines.map(lambda s: (s, 1))\n",
    "\n",
    "# Reduce the pairs by key (the lines), summing the counts for each unique line\n",
    "# Cache the resulting RDD to optimize subsequent actions\n",
    "counts = pairs.reduceByKey(lambda a, b: a + b).cache()\n",
    "\n",
    "# Count the number of unique lines (keys) in the RDD\n",
    "counts.count()\n",
    "\n",
    "display(counts.collect()) # Collect the RDD to the driver and display the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "16f7e485-df64-4c17-9165-474514ed51ec",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TfajHIRsejog",
    "outputId": "f4a9ea5d-ef0c-4ff2-d098-9ad60c43560d"
   },
   "outputs": [],
   "source": [
    "# SortByKey\n",
    "\n",
    "# Sort the RDD 'counts' by key (the lines) and store the result in 'sorted'\n",
    "sorted = counts.sortByKey()\n",
    "\n",
    "# Collect the sorted RDD to the driver and display the result\n",
    "display(sorted.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "347aab67-67bb-4464-9e57-61b087eafb1a",
     "showTitle": false,
     "title": ""
    },
    "id": "a6c2qVSLj4Cy"
   },
   "source": [
    "#### Example 5 - Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b23ee59-5ad4-460f-9968-3ab597a3c799",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fDwoLMbbdGPN",
    "outputId": "66cddc21-ea01-4a16-a989-dbc5ba8f6194"
   },
   "outputs": [],
   "source": [
    "# Filter\n",
    "\n",
    "# Filter the RDD 'textFile' to include only lines that contain the word \"the\"\n",
    "linesWithSpark = textFile.filter(lambda line: \"the\" in line)\n",
    "\n",
    "# Count the number of lines that contain the word \"the\" and display the result\n",
    "display(linesWithSpark.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0d562cf2-9f09-4afc-91ff-7296ce588bfd",
     "showTitle": false,
     "title": ""
    },
    "id": "ngS6b5jUfYen"
   },
   "source": [
    "#### Exercise 3 - Word count\n",
    "Get the word count for the file `frankenstein.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b849373e-2e2d-4801-abd6-07d0bd4fa05b",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wqbwlZn8fVNd",
    "outputId": "4ed4f385-2afd-4de4-c92d-43941d07e79b"
   },
   "outputs": [],
   "source": [
    "# Load the text file 'frankenstein.txt' into an RDD named 'words'\n",
    "words = spark.sparkContext.textFile(\"file:/databricks/driver/frankenstein.txt\")\n",
    "\n",
    "# Split each line into words, creating a flattened RDD of words\n",
    "words.flatMap(lambda x: x.split(\" \")) \\\n",
    ".map(lambda s: (s, 1)) \\\n",
    ".reduceByKey(lambda a, b: a + b) \\\n",
    ".map(lambda x: (x[1], x[0])) \\\n",
    ".sortByKey(False) \\\n",
    ".collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bda11d65-ccaf-4f72-8a07-2e26112868c0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "1. **Flattening and Mapping**:\n",
    "   - `.flatMap(lambda x: x.split(\" \"))`: This operation splits each line of text into words based on spaces, creating a new RDD where each element is a word.\n",
    "   - `.map(lambda s: (s, 1))`: This maps each word to a tuple `(word, 1)`. This transformation prepares the data for the next step, where we will count the occurrences of each word.\n",
    "\n",
    "2. **Reducing by Key**:\n",
    "   - `.reduceByKey(lambda a, b: a + b)`: This reduces the tuples by key (the word), summing up the counts (values) for each word. After this operation, each unique word will have a count representing how many times it appeared in the text.\n",
    "\n",
    "3. **Swapping and Sorting**:\n",
    "   - `.map(lambda x: (x[1], x[0]))`: This swaps the position of each tuple so that the count is the key and the word is the value. This transformation is done to prepare for sorting by the word count.\n",
    "   - `.sortByKey(False)`: This sorts the RDD by the key (the counts) in descending order (`False` indicates descending order). Now, the RDD elements are sorted such that words with higher counts appear first.\n",
    "\n",
    "4. **Collecting the Result**:\n",
    "   - `.collect()`: This action collects all the elements of the RDD (now sorted by word count) to the driver node. The result is returned as a list of tuples, where each tuple contains the count of occurrences and the corresponding word.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d6d78f64-cfee-4176-9510-15529a9eb5ee",
     "showTitle": false,
     "title": ""
    },
    "id": "034ZWkexhXQF"
   },
   "source": [
    "#### Exercise 4 - Get top 10 words\n",
    "Get TOP 10 of the words with more than 4 characters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "166a0c67-7962-4fa8-b982-6e0bea2842ad",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5QjYLZ0MgJ1_",
    "outputId": "8bbbb07a-e156-4797-92fa-c4c33a9fbf62"
   },
   "outputs": [],
   "source": [
    "words \\\n",
    ".flatMap(lambda line: line.split(\" \")) \\\n",
    ".filter(lambda word: len(word) > 4) \\\n",
    ".map(lambda word: (word, 1)) \\\n",
    ".reduceByKey(lambda a, b: a + b) \\\n",
    ".map(lambda x: (x[1], x[0])) \\\n",
    ".sortByKey(False) \\\n",
    ".take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b2663c12-35de-46d9-882e-318c57f4b6fc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "- `.flatMap(lambda line: line.split(\" \"))`: This operation splits each line into words based on spaces and creates a new RDD where each element is a word.\n",
    "- `.filter(lambda word: len(word) > 4)`: This filters out words whose length is less than or equal to 4 characters. Only words longer than 4 characters will pass through to the next step.\n",
    "- `.map(lambda word: (word, 1))`: This maps each word to a tuple `(word, 1)`, where `1` represents the count of occurrences of that word.\n",
    "- `.reduceByKey(lambda a, b: a + b)`: This reduces the tuples by key (the word), summing up the counts (values) for each word. After this operation, each unique word will have a count representing how many times it appeared in the text, but only for words longer than 4 characters.\n",
    "- `.map(lambda x: (x[1], x[0]))`: This swaps the position of each tuple so that the count is the key and the word is the value. This transformation is done to prepare for sorting by the word count.\n",
    "- `.sortByKey(False)`: This sorts the RDD by the key (the counts) in descending order (`False` indicates descending order). Now, the RDD elements are sorted such that words with higher counts appear first.\n",
    "- `.take(10)`: This action takes the top 10 elements from the RDD. These elements represent the 10 most frequent words longer than 4 characters, sorted by their frequency.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14b27220-7b2a-4986-9a40-608ba34498bc",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "47aWVH1K8X15",
    "outputId": "ed592106-da1f-470a-f32b-6c36ddf7cecb"
   },
   "outputs": [],
   "source": [
    "words \\\n",
    ".flatMap(lambda line: line.split(\" \")) \\\n",
    ".filter(lambda word: len(word) > 4) \\\n",
    ".map(lambda word: (word, 1)) \\\n",
    ".reduceByKey(lambda a, b: a + b) \\\n",
    ".top(10, key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f803aaaa-8cf7-4ee6-bc9b-1b901ef3f930",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "- `flatMap(lambda line: line.split(\" \"))`: Splits each line of text into words based on spaces and creates a new RDD where each element is a word.\n",
    "\n",
    "- `filter(lambda word: len(word) > 4)`: Filters out words whose length is less than or equal to 4 characters. Only words longer than 4 characters are retained.\n",
    "\n",
    "- `map(lambda word: (word, 1))`: Maps each word to a tuple `(word, 1)`, where `1` represents the count of occurrences of that word.\n",
    "\n",
    "- `reduceByKey(lambda a, b: a + b)`: Reduces the tuples by key (the word), summing up the counts (values) for each word. After this operation, each unique word will have a count representing how many times it appeared in the text.\n",
    "\n",
    "- `.top(10, key=lambda x: x[1])`: Retrieves the top 10 elements from the RDD based on the specified key function `lambda x: x[1]`. Here, `x[1]` denotes the count associated with each word tuple `(word, count)`. The elements are retrieved in descending order of their counts.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "24ff67cf-ac08-4ed0-bef7-85d9e5a41008",
     "showTitle": false,
     "title": ""
    },
    "id": "6IqqLY6PRtGg"
   },
   "source": [
    "## Key/Value Pair RDD\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f33586fb-d58a-493b-ab32-d8e4637f4d82",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "- Spark provides specialized operations for RDDs that store data as (key, value) pairs, often referred to as Pair RDDs.\n",
    "- These operations enable efficient parallel processing operations on each key and aggregation across the network. For example, transformations like `reduceByKey()` aggregate data locally on each partition before shuffling across the network, optimizing performance for tasks such as counting or aggregating values by key."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "58940f91-74e7-4487-99df-aa25bd00e2f4",
     "showTitle": false,
     "title": ""
    },
    "id": "Qq6NhwCXRl7n"
   },
   "source": [
    "### Example 6 - Create the RDD and remove the header\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b4f25ac-9e14-4c2d-a84e-f59c85d2a524",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R04SZu82R1ui",
    "outputId": "7e137bfc-74ce-4e30-fbb0-dc652ededfbf"
   },
   "outputs": [],
   "source": [
    "# Load the text file 'characters.csv' into an RDD named 'charac_sw'\n",
    "charac_sw = spark.sparkContext.textFile(\"file:/databricks/driver/characters.csv\")\n",
    "\n",
    "# Load the text file 'planets.csv' into an RDD named 'planets_sw'\n",
    "planets_sw = spark.sparkContext.textFile(\"file:/databricks/driver/planets.csv\")\n",
    "\n",
    "# Take the first 10 elements from the RDD 'charac_sw' and display them\n",
    "charac_sw.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52ca1b43-f126-4ceb-b09b-65346d3a21c2",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_fuMXUgnS_Rb",
    "outputId": "5badfc87-ce10-430d-f0b6-437e1465eaac"
   },
   "outputs": [],
   "source": [
    "# Take the first 10 elements from the RDD 'planets_sw' and display them\n",
    "planets_sw.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "80d23e69-f588-4b47-bd98-6665a4b617f2",
     "showTitle": false,
     "title": ""
    },
    "id": "xbYKGuvxPiqb"
   },
   "outputs": [],
   "source": [
    "from itertools import islice\n",
    "\n",
    "# Remove the header from 'charac_sw' RDD using mapPartitionsWithIndex\n",
    "charac_sw_noheader = charac_sw.mapPartitionsWithIndex(\n",
    "    lambda idx, it: islice(it, 1, None) if idx == 0 else it)\n",
    "\n",
    "# Remove the header from 'planets_sw' RDD using mapPartitionsWithIndex\n",
    "planets_sw_noheader = planets_sw.mapPartitionsWithIndex(\n",
    "    lambda idx, it: islice(it, 1, None) if idx == 0 else it)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8943547c-412f-4006-807e-dcaf814cb86c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "This operation removes the header from the RDD by skipping the first element in the first partition:\n",
    "\n",
    "- `mapPartitionsWithIndex` allows processing each partition of the RDD with an index.\n",
    "- `lambda idx, it: islice(it, 1, None) if idx == 0 else it`: \n",
    "  - For the partition with index `idx == 0` (first partition), `islice(it, 1, None)` skips the first element (header) and returns the rest of the elements.\n",
    "  - For other partitions (`else it`), it returns all elements unchanged."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7c392460-4652-4703-a7d8-16bdb8b384c2",
     "showTitle": false,
     "title": ""
    },
    "id": "m1BEEwqpTREy"
   },
   "source": [
    "### Exercise 5 - Join Pair RDDs\n",
    "Get a list of the population of the planet each Star Wars character belongs to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb868927-af54-487e-bfef-9ccfff4376a2",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "djKKpF55Npev",
    "outputId": "2a625110-5c0a-46e1-878d-57761f718262"
   },
   "outputs": [],
   "source": [
    "# Create pairs of (planet_name, climate) from 'planets_sw_noheader' RDD\n",
    "planets_sw_pair = planets_sw_noheader \\\n",
    ".map(lambda line: line.split(\";\")) \\\n",
    ".map(lambda x: (x[0], x[8]))\n",
    "\n",
    "# Create pairs of (character_name, planet_name) from 'charac_sw_noheader' RDD\n",
    "characters_sw_pair = charac_sw_noheader \\\n",
    ".map(lambda line: line.split(\",\")) \\\n",
    ".map(lambda x: (x[8], x[0]))\n",
    "\n",
    "# Join 'characters_sw_pair' and 'planets_sw_pair' RDDs on planet_name\n",
    "# Retain only distinct records and select the first 10 records\n",
    "characters_sw_pair\\\n",
    ".join(planets_sw_pair)\\\n",
    ".map(lambda x: (x[0], x[1][0], x[1][1]))\\\n",
    ".distinct()\\\n",
    ".take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bdc79438-f248-4bb3-88cc-d891845da287",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Create pairs of (planet_name, climate) from 'planets_sw_noheader' RDD: \n",
    "- `.map(lambda line: line.split(\";\"))`: splits each `line` into a list of strings using `\";\"` as the delimiter.\n",
    "- `.map(lambda x: (x[0], x[8]))`: creates a new RDD where each element (`x`) is transformed into a tuple `(x[0], x[8])` representing the key-value pair.\n",
    "     - `x[0]` likely corresponds to the first column of the CSV data, representing the planet name.\n",
    "     - `x[8]` likely corresponds to the ninth column of the CSV data, representing the climate of the planet.\n",
    "\n",
    "Create pairs of (character_name, planet_name) from 'charac_sw_noheader' RDD: similar to the above.\n",
    "\n",
    "Join 'characters_sw_pair' and 'planets_sw_pair' RDDs on planet_name:\n",
    "- `characters_sw_pair.join(planets_sw_pair)`: This operation joins two RDDs, `characters_sw_pair` and `planets_sw_pair`, based on their keys. Specifically, it joins them on the `planet_name` key, assuming `characters_sw_pair` contains tuples of `(character_name, planet_name)` and `planets_sw_pair` contains tuples of `(planet_name, climate)`.\n",
    "- `.map(lambda x: (x[0], x[1][0], x[1][1]))`: After joining, each element `x` in the resulting RDD represents a tuple `(planet_name, (character_name, climate))`. This mapping rearranges the tuple to `(character_name, planet_name, climate)`, extracting the necessary information for further analysis or display.\n",
    "- `.distinct()`: This removes duplicate tuples from the RDD. Each tuple is considered unique based on its entire structure `(character_name, planet_name, climate)`.\n",
    "- `.take(10)`: collects the first 10 elements from the RDD.\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 2309605403313973,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "0 Spark Introduction",
   "widgets": {}
  },
  "colab": {
   "collapsed_sections": [
    "yzU_4EjAjZgh"
   ],
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
