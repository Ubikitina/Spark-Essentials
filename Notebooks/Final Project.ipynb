{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fb187e7c-16bf-4efd-bdf3-7bf2ce4d53c5",
     "showTitle": false,
     "title": ""
    },
    "id": "Uk5ghOBCxPlT"
   },
   "source": [
    "# Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "73afcd50-40e2-4e13-808c-35039a0c9e6c",
     "showTitle": false,
     "title": ""
    },
    "id": "61qYFUD1ysQX"
   },
   "source": [
    "We have datasets corresponding to a **list of health inspections in establishments** (restaurants, supermarkets, etc.), along with their respective health risk. We have another dataset that shows a **description of said risk**.\n",
    "\n",
    "**The goal is to load these datasets under specific requirements and manipulate them according to the instructions of each exercise.**\n",
    "\n",
    "All necessary operations are described in the exercises, although additional tasks carried out by the student on their own initiative will be appreciated. The use of the DataFrame API will also be valued."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3a11d615-dcf1-4074-97a0-c07ea36e0d5d",
     "showTitle": false,
     "title": ""
    },
    "id": "8Z0h3dF9Vg4X"
   },
   "source": [
    "# Download Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b13db421-61d6-43df-9aba-16a9d29d9997",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sh \n",
    "curl -O 'https://raw.githubusercontent.com/masfworld/datahack_docker/master/zeppelin/data/food_inspections_lite.csv'\n",
    "curl -O 'https://raw.githubusercontent.com/masfworld/datahack_docker/master/zeppelin/data/risk_description.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f9992d60-96b6-43cd-8e2d-9c52dd718662",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Copy the local file food_inspections_lite.csv from the driver node to DBFS (Databricks File System) under /dataset/\n",
    "dbutils.fs.cp('file:/databricks/driver/food_inspections_lite.csv','dbfs:/dataset/food_inspections_lite.csv')\n",
    "\n",
    "# Copy the local file risk_description.csv from the driver node to DBFS (Databricks File System) under /dataset/\n",
    "dbutils.fs.cp('file:/databricks/driver/risk_description.csv','dbfs:/dataset/risk_description.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a29800e4-d003-4d6e-966d-0e98d8fff444",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# List all files and directories under the DBFS directory /dataset/\n",
    "dbutils.fs.ls('/dataset/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3c36bd4f-9e6b-4e0d-ad7b-ba3aeb534386",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "We do a `head` to see the content of the datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "93ef6217-4b05-4121-b9da-799c1aa3a3b5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.head(\"dbfs:/dataset/food_inspections_lite.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6da1d41e-10fd-40cc-8be2-212259e52df1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.head(\"dbfs:/dataset/risk_description.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c57ec8fd-5255-4a35-b3a0-a1cfd42dae9b",
     "showTitle": false,
     "title": ""
    },
    "id": "prvVhMD4a5o7"
   },
   "source": [
    "# Exercise 1\n",
    "---\n",
    "\n",
    "1. **Create two dataframes, one from the file `food_inspections_lite.csv` and another from `risk_description.csv`.**\n",
    "2. **Convert these two dataframes to delta tables.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6f0da4ef-5ea5-4e76-95d2-7b6b9609a139",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Create the dataframe from `food_inspections_lite.csv`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "daff7c65-9f34-4e27-a1df-a6f8d86d80cd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Cargar el archivo food_inspections_lite.csv en un DataFrame\n",
    "file_path_food = '/dataset/food_inspections_lite.csv'\n",
    "\n",
    "food_df = spark.read.format(\"csv\") \\\n",
    "  .option(\"sep\", \",\") \\\n",
    "  .option(\"inferSchema\", \"true\") \\\n",
    "  .option(\"header\", \"true\") \\\n",
    "  .load(file_path_food)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cdd548bb-3236-4bf9-bf78-9f7cf83c1242",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Rename the columns and verify that the schema is adequate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1c012d0-7c33-402f-ac42-4108c2468913",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Rename the \"License #\" column to \"License number\".\n",
    "food_df = food_df.withColumnRenamed(\"License #\", \"License number\")\n",
    "\n",
    "# Rename the columns by replacing spaces with underscores.\n",
    "new_columns = [col_name.replace(\" \", \"_\") for col_name in food_df.columns]\n",
    "\n",
    "# Rename the columns in the DataFrame\n",
    "food_df = food_df.toDF(*new_columns)\n",
    "\n",
    "# Verify new column names\n",
    "food_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b401e1ab-c557-4de4-b14c-6a054645a61b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Display the first 3 lines of the DataFrame food_df\n",
    "food_df.show(3, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7ad83b83-2fa6-4cb3-9dc4-ea2f88446171",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "We are going to clean up the `Risk` column that will be used later in the exercises. To do this, let's list the values it has and the count of each one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d2cae359-b464-404c-b935-d7a402579e4c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Grouping by 'Risk' column and counting the occurrences\n",
    "risk_counts = food_df.groupBy('Risk').count()\n",
    "\n",
    "# Showing the results\n",
    "risk_counts.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e4d236e2-4c9e-42d2-99d4-953caeb95b8c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "We see that there are `null` values and `all` values. The correct values are `Risk 1 (High)`, `Risk 2 (Medium)`, and `Risk 3 (Low)`. Therefore, we will make the following modifications:\n",
    "- We will remove the `null` values since we are unable to trace them to any of the correct values.\n",
    "- We will change the `All` values to `Risk 1 (High)`, considering that \"All\" means they have received the highest risk score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "23f76106-996f-46c0-8298-b3577a22a96c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Drop rows where the 'Risk' column has null values\n",
    "df_food_drop_risk_nulls = food_df.dropna(subset=['Risk'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7acad515-4193-46c1-a547-139c8f6a940b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when\n",
    "# Replacing \"All\" with \"Risk 1 (High)\"\n",
    "food_df_clean = df_food_drop_risk_nulls.withColumn('Risk', when(col('Risk') == 'All', 'Risk 1 (High)').otherwise(col('Risk')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d340d7b9-f1fd-42d0-81ec-b31088b9425d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Group by 'Risk' and count the occurrences\n",
    "risk_counts = food_df_clean.groupBy('Risk').count()\n",
    "\n",
    "# Show the results\n",
    "risk_counts.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5072e0e1-6929-4e61-896c-3168fe56a8b1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Create the dataframe from `risk_description.csv`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f8fb6cf-3a99-4ba1-baae-b8252e5033ed",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load the risk_description.csv file in another DataFrame\n",
    "file_path_risk = '/dataset/risk_description.csv'\n",
    "risk_df = spark.read.format(\"csv\") \\\n",
    "  .option(\"sep\", \",\") \\\n",
    "  .option(\"inferSchema\", \"true\") \\\n",
    "  .option(\"header\", \"true\") \\\n",
    "  .load(file_path_risk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f0820bb-e514-479d-8833-3c191f65cd92",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Show all lines of the DataFrame risk_df\n",
    "risk_df.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0ea2bf46-9390-4b55-9246-b5bcb27b428b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Conversion of the DataFrame `food_df_clean` to a Delta Table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4fa3d820-fa93-4732-af01-b649840c9b7b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from delta.tables import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Define the Delta Lake path for the 'food' table and remove the directory at FOOD_DELTA_PATH recursively, if it exists\n",
    "FOOD_DELTA_PATH = \"/mnt/delta/food\"\n",
    "dbutils.fs.rm(FOOD_DELTA_PATH, recurse=True)\n",
    "\n",
    "# Write the DataFrame to the specified Delta path\n",
    "food_df_clean.write.format(\"delta\").save(FOOD_DELTA_PATH)\n",
    "\n",
    "# Drop the table if it already exists\n",
    "spark.sql(\"DROP TABLE IF EXISTS food\")\n",
    "\n",
    "# Create a Delta table named 'food' using the data saved at FOOD_DELTA_PATH\n",
    "spark.sql(\"CREATE TABLE food USING DELTA LOCATION \\'\" + FOOD_DELTA_PATH + \"\\'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "98de4ed5-ee7b-45cb-bae8-1ef341250255",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT *\n",
    "FROM \n",
    "  food\n",
    "LIMIT 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f60852c6-d1b7-47be-9249-f6e85f0f18d4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT COUNT(*) AS total_rows FROM food"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3b18bd02-38b3-4c7a-8701-c5a77bdbcb27",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Conversion of the DataFrame `risk_df` to a Delta Table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "baf8363e-00c9-4f60-9de0-66da402456ed",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define the Delta Lake path for the 'risk' table and remove the directory at RISK_DELTA_PATH recursively, if it exists\n",
    "RISK_DELTA_PATH = \"/mnt/delta/risk\"\n",
    "dbutils.fs.rm(RISK_DELTA_PATH, recurse=True)\n",
    "\n",
    "# Write the DataFrame to the specified Delta path\n",
    "risk_df.write.format(\"delta\").save(RISK_DELTA_PATH)\n",
    "\n",
    "# Drop the table if it already exists\n",
    "spark.sql(\"DROP TABLE IF EXISTS risk\")\n",
    "\n",
    "# Create a Delta table named 'risk' using the data saved at RISK_DELTA_PATH\n",
    "spark.sql(\"CREATE TABLE risk USING DELTA LOCATION \\'\" + RISK_DELTA_PATH + \"\\'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7783ee47-abd9-4d21-9d41-e4818c76f18d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT *\n",
    "FROM \n",
    "  risk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "94d26822-a808-45a1-9f43-67a659891203",
     "showTitle": false,
     "title": ""
    },
    "id": "4-HOEezxVnCe"
   },
   "source": [
    "# Exercise 2\n",
    "**Obtain the number of distinct inspections with high `Risk 1 (High)`.**\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e919fcf-dcf8-4c39-95af-bd677ea39f24",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PJ88z1pQV9WF",
    "outputId": "a99b4b66-0c0e-4476-d44e-3f8b3dc5f307"
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT COUNT(DISTINCT Inspection_ID) AS num_inspections_high_risk\n",
    "FROM food\n",
    "WHERE Risk = 'Risk 1 (High)'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ea37802e-9b95-49d0-a584-f24b7fda1b9b",
     "showTitle": false,
     "title": ""
    },
    "id": "3R1kpKmIXi-h"
   },
   "source": [
    "\n",
    "# Exercise 3\n",
    "**From the dataframes loaded above, obtain a table with the following columns:<br>**\n",
    "1. `DBA Name`\n",
    "2. `Facility Type`\n",
    "3. `Risk`\n",
    "4. `Risk description`\n",
    "\n",
    "---\n",
    "I will use PySpark's `join` function to combine both DataFrames based on the `Risk` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "57d382c2-4408-485f-81f7-dac4b6aa1466",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "crVCDIcYaFlu",
    "outputId": "b16795c6-4f5b-497f-a7ae-4cf565ac979a"
   },
   "outputs": [],
   "source": [
    "# Create a new column in food_df_clean to map the values of Risk to risk_id\n",
    "food_df_mapped = food_df_clean.withColumn(\"risk_id\", \n",
    "                                    when(food_df_clean[\"Risk\"].contains(\"High\"), 1)\n",
    "                                    .when(food_df_clean[\"Risk\"].contains(\"Medium\"), 2)\n",
    "                                    .when(food_df_clean[\"Risk\"].contains(\"Low\"), 3)\n",
    "                                    .otherwise(None))\n",
    "\n",
    "# Perform the link between food_df_mapped and risk_df\n",
    "combined_df = food_df_mapped.join(risk_df, food_df_mapped[\"risk_id\"] == risk_df[\"risk_id\"], \"inner\") \\\n",
    "                            .select(food_df_mapped[\"DBA_Name\"],\n",
    "                                    food_df_mapped[\"Facility_Type\"],\n",
    "                                    food_df_mapped[\"Risk\"],\n",
    "                                    risk_df[\"description\"].alias(\"Risk_description\"))\n",
    "\n",
    "# Show the result\n",
    "combined_df.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b089ed46-7d87-4edd-89f3-ad411ef5191c",
     "showTitle": false,
     "title": ""
    },
    "id": "ZX1ahdYyb2L9"
   },
   "source": [
    "# Exercise 4\n",
    "**Access the Spark UI to view the execution plan of the previous exercise (exercise 3). Describe each of the pieces/boxes that make up the execution plan (a brief one-line description per box will be sufficient).**\n",
    "\n",
    "---\n",
    "In Apache Spark, a \"**job**\" refers to a unit of work that is sent to the cluster to be executed. Each job can consist of one or more \"**stages**\", where a stage is a processing phase that includes a set of tasks that can be executed in parallel on the cluster nodes.\n",
    "\n",
    "The execution of exercise 3 indicates that Spark has executed two **jobs**:\n",
    "- **Job 41 (Stages: 1/1)**: Corresponds to the creation and transformation of the `food_df_mapped` DataFrame using `withColumn` and the `when` conditions.\n",
    "\n",
    "- **Job 42 (Stages: 1/1, 1 skipped)**: Corresponds to the join operation (`join`) between `food_df_mapped` and `risk_df`, followed by the column selection. The \"1 skipped\" indicates that Spark has optimized the execution by detecting that some data was already available in cache or in the same state needed for the previous job.\n",
    "\n",
    "\n",
    "## Job 41 - withColumn\n",
    "\n",
    "![Details for Job 41](https://github.com/Ubikitina/Spark-Essentials/blob/main/Notebooks/img/04_01.png?raw=true)\n",
    "\n",
    "![Details for Stage 50](https://github.com/Ubikitina/Spark-Essentials/blob/main/Notebooks/img/04_02.png?raw=true)\n",
    "\n",
    "\n",
    "- **Scan csv (CSV file scan):** Spark starts by scanning the input CSV file (`food_inspections_lite.csv`) to read the data into an RDD. \n",
    "  - `FileScanRDD` represents the RDD created from reading the CSV file.\n",
    "  - `MapPartitionsRDD`: represents the RDD resulting from partitioning the data read from the CSV file.\n",
    "\n",
    "- **WholeStageCodegen (Single-stage code generation optimization):** is a physical query optimization in Spark SQL that merges multiple physical operators into a single Java function. Simply put, in this step, the calculations written in DataFrames are computed to generate the Java code to build the underlying RDDs, optimizing the execution of the transformations defined in the code (`withColumn`).\n",
    "\n",
    "- **Exchange (Data exchange or redistribution):** data partitions are exchanged or redistributed to ensure that the `withColumn` transformation is correctly applied across all data partitions.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Job 42 - Join\n",
    "\n",
    "![Details for Job 42](https://github.com/Ubikitina/Spark-Essentials/blob/main/Notebooks/img/04_03.png?raw=true)\n",
    "\n",
    "![Details for Stage 52](https://github.com/Ubikitina/Spark-Essentials/blob/main/Notebooks/img/04_04.png?raw=true)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0695a505-09cf-4e51-b412-14d59b08cf47",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "**Stage 51 (skipped):** This stage was skipped in the DAG log, possibly because it was already executed previously. It is a DAG very similar to the one explained earlier.\n",
    "\n",
    "**Stage 52:**\n",
    "- **Scan csv:** Reading the `risk_df` file.\n",
    "  - `FileScanRDD`: Represents the RDD of the CSV file read and processed as an RDD.\n",
    "  - `MapPartitionsRDD`: Represents the RDD resulting from partitioning the data read from the CSV file.\n",
    "- **ShuffleQueryStage:** Receives the data from Stage 51 and performs a shuffle operation to organize and prepare them for the subsequent Join operation.\n",
    "- **WholeStageCodegen:** This is the code optimization stage, improving the efficiency of executing the `join`. It includes several `MapPartitionsRDD` stages to organize the data into partitions and a `CartesianRDD` stage corresponding to the Join itself, as the Cartesian transformation generates a Cartesian product of two RDDs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fff4369c-a922-4910-a85d-d4e0b7b28803",
     "showTitle": false,
     "title": ""
    },
    "id": "2sQNdQ2Gbz4p"
   },
   "source": [
    "# Exercise 5\n",
    "**1. For each establishment (column `DBA Name`) and its result (column `Results`), get the number of inspections it has had.**<br><br>\n",
    "**2. Get the two establishments (`DBA Name`) that have had the most inspections for each result.**<br><br>\n",
    "**3. Save the results from point 2 in a new Delta table called `inspections_results`.**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fe3ef8c1-586a-4d2b-b11a-2ee905eb4b46",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**1. For each establishment (column `DBA Name`) and its result (column `Results`), get the number of inspections it has had.**<br><br>\n",
    "This exercise can be done using either a DataFrame or a Delta Table. Considerations to keep in mind when making the choice:\n",
    "- If we are working with large volumes of data and need features such as ACID transactions, version management, and Delta Log, then using a Delta Table would be highly recommended. This allows maintaining data integrity and having the capability to perform historical operations and data recovery efficiently.\n",
    "\n",
    "- On the other hand, if our needs are more oriented towards efficiently manipulating data in memory and we do not require the persistence and advanced management offered by Delta Lake, a DataFrame would be more suitable due to its flexibility and ease of use.\n",
    "\n",
    "In this case, I will perform the execution using both:\n",
    "\n",
    "Option 1: Using DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2dc5acf0-bda0-4605-9ead-f628ac9908f0",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TLFgHfWp0pbL",
    "outputId": "0c1d2ade-e00c-4ffa-f1fd-ac650f7d4ad6"
   },
   "outputs": [],
   "source": [
    "# Calculate the number of inspections by `DBA Name` and `Results`.\n",
    "inspections_count = food_df_clean.groupBy(\"DBA_Name\", \"Results\") \\\n",
    "                           .agg(count(\"*\").alias(\"num_inspecciones\")) \\\n",
    "                           .orderBy(col(\"num_inspecciones\").desc())\n",
    "\n",
    "# Show the result\n",
    "inspections_count.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f050cada-7e5e-4487-b85c-06aa83ac8431",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Option 2: Using the Delta Table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c9b0ad2-e032-433a-9c0d-8b5e30d94d27",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT `DBA_Name`, `Results`, COUNT(*) AS num_inspecciones\n",
    "FROM food\n",
    "GROUP BY `DBA_Name`, `Results`\n",
    "ORDER BY `num_inspecciones` DESC;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8f69e4b4-3e5d-4a47-bc16-0322181d26bf",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**2. Get the two locations (`DBA Name`) that have had the most inspections for each of the results**<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be36e6ba-5060-4b01-a030-4ba6187e6add",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Define a window partitioned by 'Results' and sorted by 'num_inspections' in descending order\n",
    "windowSpec = Window.partitionBy(\"Results\").orderBy(col(\"num_inspecciones\").desc())\n",
    "\n",
    "# Add a ranking column based on the number of inspections per result\n",
    "# Filter to get only the two most frequent locations for each result\n",
    "# Sort by 'Results' and 'rank'.\n",
    "ranked_inspections = inspections_count.withColumn(\"rank\", rank().over(windowSpec))\\\n",
    "      .filter(col(\"rank\") <= 2)\\\n",
    "      .orderBy(\"Results\", \"rank\")\n",
    "\n",
    "# Show the final result \n",
    "ranked_inspections.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8aa7bf34-4f3a-4824-90f1-cba13aeca230",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "In the result above, we see that some results have ties, and therefore, we get three establishments. For example, `No Entry` has `LANS` in the first position with 5 inspections, but in the second position, there is a tie between `FORK` and `LA PENA RESTAURANTE`, as each has 4 inspections with this result.\n",
    "\n",
    "**3. Save the results from point 2 in a new Delta table called `inspections_results`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99666cfc-84ea-47af-9b3d-e3cbe5290f59",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define the Delta Lake path for the 'inspections_results' table and delete the directory in INSPECTION_RESULTS_DELTA_PATH recursively, if it exists.\n",
    "INSPECTION_RESULTS_DELTA_PATH = \"/mnt/delta/inspections_results\"\n",
    "dbutils.fs.rm(INSPECTION_RESULTS_DELTA_PATH, recurse=True)\n",
    "\n",
    "# Write the DataFrame to the specified Delta path\n",
    "ranked_inspections.write.format(\"delta\").save(INSPECTION_RESULTS_DELTA_PATH)\n",
    "\n",
    "# Create a Delta table named \"inspections_results\" using data stored in INSPECTION_RESULTS_DELTA_PATH\n",
    "spark.sql(\"CREATE TABLE inspections_results USING DELTA LOCATION \\'\" + INSPECTION_RESULTS_DELTA_PATH + \"\\'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58aff4fd-e0e5-4b77-8640-b4305ec8c4cc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT *\n",
    "FROM \n",
    "  inspections_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "42d86456-6a93-4b5d-aed1-dd2105921c8d",
     "showTitle": false,
     "title": ""
    },
    "id": "04y2Ys6L0wTU"
   },
   "source": [
    "# Exercise 6\n",
    "1. **Update the delta table of the previous exercise `inspections_results`, specifying `DBA_Name = error`**<br>\n",
    "2. **Restore the table to its original state**\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c81138de-0676-45c7-86b9-6cf497fb1e64",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "First we will see the details of the metadata history of the `inspection_results` table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e8987f1d-b2cc-4ce8-b6b6-3fdfb430ae8e",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U1Pp9VwU1DnK",
    "outputId": "b56672fa-5a19-4f6a-81de-522b931e433f"
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "DESCRIBE HISTORY inspections_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2075e06e-2313-4141-bf90-2e1e2894c39a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Now we update the delta table by specifying `DBA_Name = Error`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eed5230b-96f0-4d11-807c-f6d8ccbdb37c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "UPDATE inspections_results SET DBA_Name = \"Error\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f9e8a9b3-f7e6-4a85-af34-11fba12c6565",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "We check that it has done so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19204064-80f8-46d8-ad75-f9c5b7d9006a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT *\n",
    "FROM inspections_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dea49144-9a84-40e5-9bef-3cc05aa26bcb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "We verify that this update is reflected in the history:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84979686-9989-48a0-998e-dfe219c90b40",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "DESCRIBE HISTORY inspections_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c41ea637-d90b-4547-89cc-8a8f6cdd75cc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "We restore the table to its original state:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df1c565a-a469-4246-a92b-4b96d0baf17b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read data from the Delta Lake format in the specified version '0'.\n",
    "inspections_results_df_v0 = spark.read \\\n",
    "  .format(\"delta\") \\\n",
    "  .option(\"versionAsOf\", \"0\") \\\n",
    "  .load(INSPECTION_RESULTS_DELTA_PATH)\n",
    "\n",
    "# Rewrite the DataFrame inspections_results_df_v0 in Delta Lake format, overwriting any existing data\n",
    "inspections_results_df_v0 \\\n",
    "  .write \\\n",
    "  .format(\"delta\") \\\n",
    "  .mode(\"overwrite\") \\\n",
    "  .save(INSPECTION_RESULTS_DELTA_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "97664910-dee8-4ef4-b544-3082a00db187",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Verify that the values have been restored correctly and check that the transaction has been recorded in the history:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf786760-f106-4a30-8915-16fa1c68fad9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT *\n",
    "FROM inspections_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f8a3c3d-b963-4857-ab41-7e99f2ed0cd6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "DESCRIBE HISTORY inspections_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a86c2cf1-c6e3-47fa-8677-f2bc8699b5e7",
     "showTitle": false,
     "title": ""
    },
    "id": "77M6b7WwsTA1"
   },
   "source": [
    "# Exercise 7\n",
    "\n",
    "**Create a Structured Streaming application that reads data from the Kafka topic `inspections`. The Kafka server URL is `35.237.99.179:9094`:**\n",
    "\n",
    "**The data from this topic is exactly the same as what we have been analyzing throughout this notebook, `Food Inspections`, so the schema is the same.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58890d9d-8318-4263-ad99-7f46cd28c324",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T2313e8Du6RX",
    "outputId": "ed179851-dc6a-4ca8-ace7-144a407a3044"
   },
   "outputs": [],
   "source": [
    "# Read streaming data from Kafka\n",
    "df = spark \\\n",
    "  .readStream \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", \"35.237.99.179:9094\") \\\n",
    "  .option(\"subscribe\", \"inspections\") \\\n",
    "  .load()\n",
    "\n",
    "# Print the schema of the DataFrame\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bfc8cd1c-4d08-408f-a62f-e3da3776c2ed",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Define the schema to parse JSON data\n",
    "schema = StructType(\n",
    "  [\n",
    "    StructField(\"Inspection ID\", StringType(), True),\n",
    "    StructField(\"DBA Name\", StringType(), True),\n",
    "    StructField(\"AKA Name\", StringType(), True),\n",
    "    StructField(\"License #\", StringType(), True),\n",
    "    StructField(\"Facility Type\", StringType(), True),\n",
    "    StructField(\"Risk\", StringType(), True),\n",
    "    StructField(\"Address\", StringType(), True),\n",
    "    StructField(\"City\", StringType(), True),\n",
    "    StructField(\"State\", StringType(), True),\n",
    "    StructField(\"Zip\", StringType(), True),\n",
    "    StructField(\"Inspection Date\", StringType(), True),\n",
    "    StructField(\"Inspection Type\", StringType(), True),\n",
    "    StructField(\"Results\", StringType(), True),\n",
    "    StructField(\"Violations\", StringType(), True),\n",
    "    StructField(\"Latitude\", StringType(), True),\n",
    "    StructField(\"Longitude\", StringType(), True),\n",
    "    StructField(\"Location\", StringType(), True)\n",
    "  ]\n",
    ")\n",
    "\n",
    "# Select the columns key, value, and timestamp, converting key and value to strings\n",
    "# Parse the value column from JSON format using the specified schema\n",
    "# Select the key, timestamp, and all columns from the parsed JSON value.\n",
    "dataset = df.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\", \"timestamp\") \\\n",
    "    .withColumn(\"value\", from_json(\"value\", schema)) \\\n",
    "    .select(col('key'), col(\"timestamp\"), col('value.*'))\n",
    "\n",
    "# Print the schema\n",
    "dataset.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "631cdd1c-188d-4ac2-be97-ec6e01f7a5ba",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Start the streaming with `writeStream`. `writeStream` configures a streaming channel in Spark Structured Streaming and initiates the execution of the streaming. The configuration applied in this case includes: append mode, storing results in memory, assigning a query name, etc.\n",
    "\n",
    "This enables real-time data processing and analysis, making the results immediately available for querying through Spark SQL or other subsequent applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5721e970-77bb-4bba-8409-3325b5a1ba6e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Specify the output mode as 'append' (only new rows added to the results table)\n",
    "# Define the output sink format as 'memory' (store the results table in memory)\n",
    "# Option to truncate long strings in the output table (set to 'false' to display the full content)\n",
    "# Assign a name to the query (to be referenced in Spark SQL)\n",
    "# Start the streaming query\n",
    "dataset.writeStream \\\n",
    " .outputMode(\"append\") \\\n",
    " .format(\"memory\") \\\n",
    " .option(\"truncate\", \"false\") \\\n",
    " .queryName(\"inspections_topic\") \\\n",
    " .start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "15efaa2f-0996-4cbd-b61c-2aed6e27852b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "We check the availability of the data by running a query in Spark SQL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c8268abf-e619-45e9-b8b7-4409dac08857",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT\n",
    "  *\n",
    "FROM\n",
    "  inspections_topic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "090c9d0b-305b-432c-a686-e87b3fca83be",
     "showTitle": false,
     "title": ""
    },
    "id": "JbyutPORhr0b"
   },
   "source": [
    "# Exercise 8\n",
    "**Based on the data source from the previous exercise, obtain the number of inspections by `Facility Type` every 5 seconds.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "03abdab4-f1a7-4345-a28a-d5ba6ab8eb01",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "We will group the data by time windows and then count the records by `Facility Type`. We will use the `.display()` method for visualization.\n",
    "\n",
    "**Note:** The `.display()` method is only available in certain development environments, such as Databricks, and is primarily designed for rapid data exploration and development. It is not recommended for use in production environments.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b404cb3-4e6d-4baf-830c-286b98dc8346",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jx_xjxBthxyO",
    "outputId": "28de96fc-241f-4ba1-bcdb-d80a27601f24"
   },
   "outputs": [],
   "source": [
    "dataset.groupBy(window(col(\"timestamp\"), \"5 seconds\"), col(\"Facility Type\")) \\\n",
    "    .count() \\\n",
    "    .display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a5247c05-8ea6-4c12-8f72-04b5efbfe91c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Instead of using `.display()` in production environments, it is preferable to save the results in persistent storage formats using the `writeStream` method.\n",
    "\n",
    "Below is the code demonstrating how to do this with `writeStream` for production environments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "24391b75-df5a-47a8-a23f-b5c1cd0014bb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Specify the output mode as 'update'\n",
    "# Define the output sink format as 'memory'\n",
    "# Set the option to truncate long strings in the output table to false\n",
    "# Assign a name to the query (to be referenced in Spark SQL)\n",
    "# Start the streaming query\n",
    "dataset.groupBy(window(col(\"timestamp\"), \"5 seconds\"), col(\"Facility Type\")) \\\n",
    "    .count() \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"update\") \\\n",
    "    .format(\"memory\") \\\n",
    "    .option(\"truncate\", \"false\") \\\n",
    "    .queryName(\"inspections_grouped_topic\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "98ed6a57-6943-46d7-ae9c-e2763d289d90",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "As in exercise 7, we check the availability of the data by running a query in Spark SQL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "701cf9c1-6044-4e2d-9958-2b6f2fe90236",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT\n",
    "  *\n",
    "FROM\n",
    "  inspections_grouped_topic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "86fcae80-04aa-4d20-892b-b813877a2b60",
     "showTitle": false,
     "title": ""
    },
    "id": "sr3vImmwxQiN"
   },
   "source": [
    "# Exercise 9\n",
    "**Based on the data source from exercise 7, obtain the number of inspections by `Results` every 5 seconds for the last 30 seconds.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "743f2cd8-6e91-40f3-8c2a-d5986174459f",
     "showTitle": false,
     "title": ""
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cwoGr9xWw5a4",
    "outputId": "1304eee1-10b0-438b-d90b-aa5e6570a596"
   },
   "outputs": [],
   "source": [
    "dataset.groupBy(window(col(\"timestamp\"), \"30 seconds\", \"5 seconds\"), col(\"Results\")) \\\n",
    "    .count() \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"update\") \\\n",
    "    .format(\"memory\") \\\n",
    "    .option(\"truncate\", \"false\") \\\n",
    "    .queryName(\"inspections_grouped_topic2\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4ac51af4-eb9a-4bd1-afb7-52378a4608f1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "As in exercise 8, we check the availability of the data by running a query in Spark SQL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b22e21e-8bff-4f5b-b119-fb9e09af59c2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT\n",
    "  *\n",
    "FROM\n",
    "  inspections_grouped_topic2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "14d4e4b5-b08f-41f9-a34b-7cf40f62b7c4",
     "showTitle": false,
     "title": ""
    },
    "id": "BNea5uVAx1DG"
   },
   "source": [
    "# Exercise 10\n",
    "1. **Update the `Results` column of the Delta table for food inspections created in exercise 1 to the value `No result`.**\n",
    "2. **Update the data in the modified table from point 1 as new items arrive in Kafka.**\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5996777d-c73c-4532-a477-e42d22ec4f04",
     "showTitle": false,
     "title": ""
    },
    "id": "fbT-_TJv0YMj"
   },
   "source": [
    "It is advisable to stop all previous streams, as the one for this exercise tends to be resource-intensive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7857b602-3266-4968-a2da-3c3bbf3426da",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**1. Update the `Results` column of the Delta table for food inspections created in exercise 1 to the value `No result`.**\n",
    "\n",
    "Before starting, we print the details of the Delta `food` table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aaa672c0-9a7c-4cca-8a77-00df3bb78d22",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "DESCRIBE FORMATTED food"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "feafb474-3411-489e-acc0-9f73fa1484bd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "And we also print an example of the values it contains:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "599d26bd-b66b-4d21-a1f0-90e0cbf35aa0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT *\n",
    "FROM \n",
    "  food\n",
    "LIMIT 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1324b98f-6509-44ab-ab41-4eb7ce3367f3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "We will now update the `Results` column to set the value `No result`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a12ad7d-e60a-4f75-9cca-438736cde34e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Update the column Results\n",
    "UPDATE food SET Results = 'No result';"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4228affe-9100-471d-a293-4615cc39d8c3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "We check that it has been updated by printing a sample of the data again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c130f550-57e1-4e08-962c-de351c075d24",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT *\n",
    "FROM \n",
    "  food\n",
    "LIMIT 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "67438ed5-98bb-41e4-a963-d788e8294e9f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**2. Update the data in the modified table from point 1 as new items arrive in Kafka**\n",
    "\n",
    "First we establish the connection to Kafka by reusing the code from exercise 7:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe0ceb19-66ee-4743-a665-bcfed3ed4d88",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Read streaming data from Kafka\n",
    "df = spark \\\n",
    "  .readStream \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", \"35.237.99.179:9094\") \\\n",
    "  .option(\"subscribe\", \"inspections\") \\\n",
    "  .load()\n",
    "\n",
    "\n",
    "# Define the schema for parsing JSON data\n",
    "schema = StructType(\n",
    "  [\n",
    "    StructField(\"Inspection ID\", StringType(), True),\n",
    "    StructField(\"DBA Name\", StringType(), True),\n",
    "    StructField(\"AKA Name\", StringType(), True),\n",
    "    StructField(\"License #\", StringType(), True),\n",
    "    StructField(\"Facility Type\", StringType(), True),\n",
    "    StructField(\"Risk\", StringType(), True),\n",
    "    StructField(\"Address\", StringType(), True),\n",
    "    StructField(\"City\", StringType(), True),\n",
    "    StructField(\"State\", StringType(), True),\n",
    "    StructField(\"Zip\", StringType(), True),\n",
    "    StructField(\"Inspection Date\", StringType(), True),\n",
    "    StructField(\"Inspection Type\", StringType(), True),\n",
    "    StructField(\"Results\", StringType(), True),\n",
    "    StructField(\"Violations\", StringType(), True),\n",
    "    StructField(\"Latitude\", StringType(), True),\n",
    "    StructField(\"Longitude\", StringType(), True),\n",
    "    StructField(\"Location\", StringType(), True)\n",
    "  ]\n",
    ")\n",
    "\n",
    "# Convert Kafka's data to the schema. To do so:\n",
    "#   - Select key, value, and timestamp columns, converting key and value to strings\n",
    "#   - Parse the value column from the JSON format using the specified schema\n",
    "#   - Select the key, the timestamp and all columns of the parsed JSON value\n",
    "dataset = df.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\", \"timestamp\") \\\n",
    "    .withColumn(\"value\", from_json(\"value\", schema)) \\\n",
    "    .select(col('key'), col(\"timestamp\"), col('value.*'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6de42f33-0023-4ca4-9950-7a3648347669",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "In this exercise, it is essential to consider that the Delta table schema to be modified contains columns typed as integers, doubles, among others. Therefore, it is necessary to properly typify the data read from Kafka. For this purpose, the following actions will be performed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e54c1088-e6a4-4826-b30a-e19d1ff74450",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType, DoubleType, DateType\n",
    "\n",
    "# Make the casting of the data types to avoid problems in the merge\n",
    "dataset \\\n",
    "  .withColumn(\"Inspection ID\", col(\"Inspection ID\").cast(IntegerType())) \\\n",
    "  .withColumn(\"License #\", col(\"License #\").cast(IntegerType())) \\\n",
    "  .withColumn(\"Zip\", col(\"Zip\").cast(IntegerType())) \\\n",
    "  .withColumn(\"Inspection Date\", to_date(col(\"Inspection Date\"), \"MM/dd/yyyy\")) \\\n",
    "  .withColumn(\"Latitude\", col(\"Latitude\").cast(DoubleType())) \\\n",
    "  .withColumn(\"Longitude\", col(\"Longitude\").cast(DoubleType()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2e0feddb-50cb-47ce-8784-e0ab1867b69d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Now we will define a function to update the Delta table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f34a92ca-f456-4e4b-b416-1eff839683b9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from delta.tables import *\n",
    "\n",
    "# Adjust the number of partitions for shuffle operations, optimizing performance based on cluster size\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"200\")  \n",
    "\n",
    "\n",
    "# Function for updating the Delta table\n",
    "def upsertToDelta(microBatchOutputDF, batchId):\n",
    "\n",
    "    # Debugging: Print the number of records in the batch\n",
    "    print(f\"Processing batch ID: {batchId} with {microBatchOutputDF.count()} records.\")\n",
    "    \n",
    "    # Load existing Delta table\n",
    "    delta_table = DeltaTable.forName(spark, \"food\")\n",
    "\n",
    "    # Debugging: Check that the Delta Table has been loaded successfully\n",
    "    record_count = delta_table.toDF().count()\n",
    "    print(f\"Total number of records in the table 'food': {record_count}\")\n",
    "\n",
    "    # Debugging: Filter the microbatch records that already exist in the 'food' table and count how many there are\n",
    "    matched_count = microBatchOutputDF.filter(\"`Inspection ID` IN (SELECT Inspection_ID FROM food)\").count()\n",
    "    print(f\"Matching records: {matched_count}\")\n",
    "\n",
    "    # Perform merge operation on Delta table\n",
    "    delta_table.alias(\"target\").merge(\n",
    "        microBatchOutputDF.alias(\"source\"),\n",
    "        \"target.Inspection_ID = cast(source.`Inspection ID` as Integer)\"\n",
    "    ).whenMatchedUpdate(set = {\n",
    "            \"target.Results\": \"source.Results\"\n",
    "    }).whenNotMatchedInsert(values={\n",
    "            \"target.Inspection_ID\": \"source.`Inspection ID`\",\n",
    "            \"target.DBA_Name\": \"source.`DBA Name`\",\n",
    "            \"target.AKA_Name\": \"source.`AKA Name`\",\n",
    "            \"target.License_number\": \"source.`License #`\",\n",
    "            \"target.Facility_Type\": \"source.`Facility Type`\",\n",
    "            \"target.Risk\": \"source.Risk\",\n",
    "            \"target.Address\": \"source.Address\",\n",
    "            \"target.City\": \"source.City\",\n",
    "            \"target.State\": \"source.State\",\n",
    "            \"target.Zip\": \"source.Zip\",\n",
    "            \"target.Inspection_Date\": \"source.`Inspection date`\",\n",
    "            \"target.Results\": \"source.Results\",\n",
    "            \"target.Violations\": \"source.Violations\",\n",
    "            \"target.Latitude\": \"source.Latitude\",\n",
    "            \"target.Longitude\": \"source.Longitude\",\n",
    "            \"target.Location\": \"source.Location\"         \n",
    "    }).execute()\n",
    "\n",
    "\n",
    "    # Debugging: Convert the Delta table to a DataFrame and counts the total number of records in the resulting DataFrame.\n",
    "    delta_table_a_df = delta_table.toDF()\n",
    "    record_count = delta_table_a_df.count()\n",
    "    print(f\"Merge completed. Total number of records in the table 'food' after the merge: {record_count}\")\n",
    "\n",
    "    # Debugging: Count the values in the 'Results' column and display the result\n",
    "    result_count = delta_table_a_df.groupBy(\"Results\").count()\n",
    "    result_count.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dcf9805b-d8ab-4b03-a656-670ecd7cad2e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Apply the `upsertToDelta` function to the stream:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a29a1f9b-5679-43ef-8e94-55c0f589e4ff",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Delete the checkpoints directory for the 'food' table, including all its contents.\n",
    "dbutils.fs.rm(\"dbfs:/mnt/delta/checkpoints/food\", recurse=True)\n",
    "\n",
    "# Configure the stream to process data and apply upsertToDelta function\n",
    "query = dataset.writeStream \\\n",
    "    .foreachBatch(upsertToDelta) \\\n",
    "    .outputMode(\"update\") \\\n",
    "    .option(\"checkpointLocation\", \"dbfs:/mnt/delta/checkpoints/food\") \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e8842772-4ca6-4ebd-8a04-ab25bb17674f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "We will check that the updates are being made by counting the values of the \"Results\" column. Executing it twice, we will see that in the first execution the count of the results different to \"No result\" is higher, and in the second one lower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b2211e4d-c9e7-41db-b9f4-27c8b5d8cb9b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT Results, COUNT(*) as Count\n",
    "FROM food\n",
    "GROUP BY Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd0cbbaa-05b1-41ed-b0c9-1ec5015252c1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT Results, COUNT(*) as Count\n",
    "FROM food\n",
    "GROUP BY Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0b55a968-dc55-4f6b-a435-4e5ffd12bef4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "We also check the total number of records, to see if new ones have been added:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ad2f127-2ef0-49c9-8570-06bea111b5ba",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT COUNT(*) AS total_lines FROM food"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a6a72ce0-cf6b-4422-98cc-5801747a6101",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**Clarification on all debugging instructions** incorporated in the `upsertToDelta` method:\n",
    "\n",
    "During the development of the merge method, I encountered several challenges:\n",
    "- It has been difficult to correctly map column names.\n",
    "- Once the mapping was completed, the execution did not update the data (due to incorrect filtering), and I couldn't identify the apparent reason for this issue.\n",
    "\n",
    "To detect the cause of the failures, it was necessary to include various debugging instructions, such as:\n",
    "\n",
    "```python\n",
    "# Debugging: Print the number of records in the batch\n",
    "print(f\"Processing batch ID: {batchId} with {microBatchOutputDF.count()} records.\")\n",
    "\n",
    "# Debugging: Check that the Delta Table has been loaded successfully\n",
    "record_count = delta_table.toDF().count()\n",
    "print(f\"Total number of records in the table 'food': {record_count}\")\n",
    "\n",
    "# Debugging: Filter the microbatch records that already exist in the 'food' table and count how many there are\n",
    "matched_count = microBatchOutputDF.filter(\"`Inspection ID` IN (SELECT Inspection_ID FROM food)\").count()\n",
    "print(f\"Matching records: {matched_count}\")\n",
    "\n",
    "...\n",
    "```\n",
    "\n",
    "The results of these instructions can be viewed in the Standard Output (stdout) of Databricks by accessing the menu:\n",
    "\n",
    "- `Compute` > Select the cluster number > `Driver Logs` tab > `Standard Output` section.\n",
    "\n",
    "This has allowed me to monitor the execution and effectively debug the process.\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 2600508906150223,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "Final Project",
   "widgets": {}
  },
  "colab": {
   "collapsed_sections": [
    "Jq9d0x1OTh2N",
    "8Z0h3dF9Vg4X"
   ],
   "name": "Evaluacion_Apache_Spark_Solutions.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
